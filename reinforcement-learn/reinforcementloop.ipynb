{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning without Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up, down, left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Environment\n",
    "Notes:\n",
    "\n",
    "0 is empty cell\\\n",
    "all tiles are displayed as $\\log_2(\\text{tile value})$ for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tile(board):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:   # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_right(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_right(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 1:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 2:  # left\n",
    "        new_board, reward = move_right(board)\n",
    "    elif direction == 3:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Game2048Env:\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(add_tile(self.board))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        old_max_tile = np.max(self.board)\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        changed = not np.array_equal(self.board, old_board)\n",
    "        if changed: # only add a tile if the board changed\n",
    "            self.board = add_tile(self.board)\n",
    "        new_max_tile = np.max(self.board)\n",
    "        reward = (new_max_tile > old_max_tile)  # reward for increasing max tile, small reward for merging\n",
    "        done = is_game_over(self.board)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board.flatten() / 17.0  # normalize log2(2^17), max tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Testing\n",
    "for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game2048Env()\n",
    "state = env.reset()\n",
    "print(\"Initial board:\")\n",
    "print(env.board)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    print(\"\\nCurrent board:\")\n",
    "    print(env.board)\n",
    "\n",
    "    move_str = input(\"Enter move (w=up, s=down, a=left, d=right, q=quit): \")\n",
    "    if move_str == 'q':\n",
    "        break\n",
    "    move_map = {'w': 0, 's': 1, 'a': 2, 'd': 3}\n",
    "    if move_str not in move_map:\n",
    "        print(\"Invalid input.\")\n",
    "        continue\n",
    "\n",
    "    action = move_map[move_str]\n",
    "    prev_max = np.max(env.board)\n",
    "    state, reward, done = env.step(action)\n",
    "    new_max = np.max(env.board)\n",
    "\n",
    "    print(f\"Action: {move_str.upper()} | Reward: {reward:.2f} | Max tile: {2 ** new_max}\")\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"\\nGame Over.\")\n",
    "print(\"Final board:\")\n",
    "print(env.board)\n",
    "print(f\"Total reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(16, 256), # 4x4 board flattened\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4)   # 4 possible actions\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game2048Env()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "memory = deque(maxlen=200000)\n",
    "BATCH_SIZE = 128    # batch size for training\n",
    "GAMMA = 0.999    # discount factor for future rewards\n",
    "EPSILON = 1.0   # exploration rate\n",
    "EPSILON_DECAY = 0.9995   # decay rate for exploration\n",
    "EPSILON_MIN = 0.05   # minimum exploration rate\n",
    "TARGET_UPDATE = 10  # how often to update the target network\n",
    "\n",
    "def sample_action(state):\n",
    "    if random.random() < EPSILON:   #if smaller than the exploration rate, choose random action\n",
    "        return random.choice(ACTIONS)\n",
    "    with torch.no_grad():   # otherwise choose the action based on the policy network\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = policy_net(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "def optimize(): # training\n",
    "    if len(memory) < BATCH_SIZE:    # not enough samples to train\n",
    "        return\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor(rewards).to(device).unsqueeze(1)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.BoolTensor(dones).to(device).unsqueeze(1)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions)\n",
    "    next_q_values = target_net(next_states).max(1, keepdim=True)[0]\n",
    "    target_q = rewards + GAMMA * next_q_values * (~dones)\n",
    "\n",
    "    loss = nn.functional.mse_loss(q_values, target_q.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    state = env.reset() #reset the game\n",
    "    total_reward = 0\n",
    "    for t in range(1000):   #start playing games\n",
    "        action = sample_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        optimize()\n",
    "        if done:\n",
    "            break\n",
    "    biggest_tile = np.max(env.board)\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "    if i % TARGET_UPDATE == 0:  #update policy network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Episode {i}, Total reward: {total_reward:.2f}, Biggest tile: {biggest_tile}, Epsilon: {EPSILON:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "dummy_input = torch.randn(1, 16).to(device)  # 4x4 flattened\n",
    "torch.onnx.export(policy_net, dummy_input, \"2048_rl.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "the package versions need to be really specific or else it crashes and burns\n",
    "\n",
    "make a new virtual env with the following:\\\n",
    "`python3.9 -m venv tfenv`\n",
    "\n",
    "run this bash script to convert to Tensorflow:\\\n",
    "`pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && onnx-tf convert -i 2048_rl.onnx -o 2048_rl_tf`\n",
    "\n",
    "and then this bash script to convert from Tensorflow to Tensorflow.js:\\\n",
    "`pip install tensorflowjs==4.18.0 && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_rl_tf/ 2048_rl_tfjs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
