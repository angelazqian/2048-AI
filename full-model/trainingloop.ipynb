{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with RL Finetuning through Self-Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for subdir in os.listdir(data_dir):\n",
    "    subdir_path = os.path.join(data_dir, subdir)\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line.strip())\n",
    "                if \"state\" in data and \"action\" in data:\n",
    "                    X.append(data[\"state\"])\n",
    "                    Y.append(data[\"action\"])\n",
    "X = np.array(X)\n",
    "X[X > 0] = np.log2(X[X > 0])    #replace with log2 for simplicity\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=26)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(Y)   #overwrite with full dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NumPy Arrays to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "X shape: torch.Size([64, 1, 4, 4])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):   #reshape to fit CNN input, -1 to auto infer batch size, 1 for single channel\n",
    "        self.X = torch.from_numpy(X.astype(np.float32)).reshape(-1, 1, 4, 4)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "   \n",
    "batch_size = 64\n",
    "\n",
    "#instantiate training and test data\n",
    "train_data = Data(X_train, y_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test, y_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#sanity check\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = 16\n",
    "hidden_dim1 = 256\n",
    "hidden_dim2 = 128\n",
    "output_dim = 4\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.layer_1 = nn.Linear(input_dim, hidden_dim1)\n",
    "#         nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"relu\")\n",
    "#         self.layer_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "#         nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"relu\")\n",
    "#         self.layer_3 = nn.Linear(hidden_dim2, output_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.nn.functional.relu(self.layer_1(x))\n",
    "#         x = torch.nn.functional.relu(self.layer_2(x))\n",
    "#         x = self.layer_3(x)\n",
    "\n",
    "#         return x\n",
    "    \n",
    "# model = NeuralNetwork(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "\n",
    "class CNN(nn.Module):   #use CNN because input is image-like (4x4 grid)\n",
    "    def __init__(self, output_dim=4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 128)  # final output size after convs\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNN(output_dim=output_dim)\n",
    "print(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch 2/30\n",
      "Epoch 3/30\n",
      "Epoch 4/30\n",
      "Epoch 5/30\n",
      "Epoch 6/30\n",
      "Epoch 7/30\n",
      "Epoch 8/30\n",
      "Epoch 9/30\n",
      "Epoch 10/30\n",
      "Epoch 11/30\n",
      "Epoch 12/30\n",
      "Epoch 13/30\n",
      "Epoch 14/30\n",
      "Epoch 15/30\n",
      "Epoch 16/30\n",
      "Epoch 17/30\n",
      "Epoch 18/30\n",
      "Epoch 19/30\n",
      "Epoch 20/30\n",
      "Epoch 21/30\n",
      "Epoch 22/30\n",
      "Epoch 23/30\n",
      "Epoch 24/30\n",
      "Epoch 25/30\n",
      "Epoch 26/30\n",
      "Epoch 27/30\n",
      "Epoch 28/30\n",
      "Epoch 29/30\n",
      "Epoch 30/30\n",
      "DONE!!! :3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "\n",
    "print(\"DONE!!! :3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85%\n",
      "Prediction distribution:\n",
      "0:  32119\n",
      "1:  33445\n",
      "2:  30383\n",
      "3:  32468\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "results = [0,0,0,0]\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get model outputs\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_test.extend(y.tolist())\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        for pred in predicted:\n",
    "            results[pred.item()] += 1\n",
    "\n",
    "print(f'Accuracy: {100 * correct // total}%')\n",
    "print(f'Prediction distribution:')\n",
    "print(f'0:  {results[0]}')\n",
    "print(f'1:  {results[1]}')\n",
    "print(f'2:  {results[2]}')\n",
    "print(f'3:  {results[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up, down, left, right\n",
    "\n",
    "def add_tile(board):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:   # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_right(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_right(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 1:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 2:  # left\n",
    "        new_board, reward = move_right(board)\n",
    "    elif direction == 3:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Game2048Env:\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(add_tile(self.board))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # old_max_tile = np.max(self.board)\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        changed = not np.array_equal(self.board, old_board)\n",
    "        if changed: # only add a tile if the board changed\n",
    "            self.board = add_tile(self.board)\n",
    "        # new_max_tile = np.max(self.board)\n",
    "        # reward = (new_max_tile > old_max_tile)  # reward for increasing max tile, small reward for merging\n",
    "        done = is_game_over(self.board)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        board = self.board.copy()\n",
    "        board = np.where(board > 0, board, 0)\n",
    "        board = board.astype(np.float32)\n",
    "        board = board.reshape(1, 1, 4, 4)\n",
    "        return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Max Tile: 6, Final Score: 712,  Baseline: 975.73, Improvement: -254.53\n",
      "Episode 100, Max Tile: 6, Final Score: 596,  Baseline: 984.17, Improvement: -380.17\n",
      "Episode 150, Max Tile: 6, Final Score: 568,  Baseline: 898.30, Improvement: -322.50\n",
      "Episode 200, Max Tile: 7, Final Score: 1392,  Baseline: 1039.05, Improvement: 367.25\n",
      "Episode 250, Max Tile: 8, Final Score: 2412,  Baseline: 1162.07, Improvement: 1270.73\n",
      "Episode 300, Max Tile: 6, Final Score: 784,  Baseline: 928.51, Improvement: -134.31\n",
      "Episode 350, Max Tile: 6, Final Score: 560,  Baseline: 931.18, Improvement: -363.58\n",
      "Episode 400, Max Tile: 7, Final Score: 1492,  Baseline: 1019.30, Improvement: 487.90\n",
      "Episode 450, Max Tile: 6, Final Score: 828,  Baseline: 982.07, Improvement: -143.97\n",
      "Episode 500, Max Tile: 7, Final Score: 1032,  Baseline: 916.30, Improvement: 126.60\n",
      "Episode 550, Max Tile: 6, Final Score: 668,  Baseline: 857.96, Improvement: -180.86\n",
      "Episode 600, Max Tile: 5, Final Score: 288,  Baseline: 923.55, Improvement: -630.05\n",
      "Episode 650, Max Tile: 7, Final Score: 1876,  Baseline: 1016.93, Improvement: 878.27\n",
      "Episode 700, Max Tile: 7, Final Score: 980,  Baseline: 1032.95, Improvement: -42.55\n",
      "Episode 750, Max Tile: 6, Final Score: 476,  Baseline: 941.14, Improvement: -458.24\n",
      "Episode 800, Max Tile: 7, Final Score: 1604,  Baseline: 1072.91, Improvement: 547.59\n",
      "Episode 850, Max Tile: 6, Final Score: 884,  Baseline: 953.55, Improvement: -58.85\n",
      "Episode 900, Max Tile: 6, Final Score: 448,  Baseline: 996.04, Improvement: -541.64\n",
      "Episode 950, Max Tile: 6, Final Score: 620,  Baseline: 1065.52, Improvement: -437.32\n",
      "Episode 1000, Max Tile: 6, Final Score: 652,  Baseline: 1034.20, Improvement: -373.60\n",
      "Episode 1050, Max Tile: 6, Final Score: 672,  Baseline: 1148.03, Improvement: -466.93\n",
      "Episode 1100, Max Tile: 7, Final Score: 1068,  Baseline: 940.10, Improvement: 139.30\n",
      "Episode 1150, Max Tile: 6, Final Score: 708,  Baseline: 1025.84, Improvement: -308.34\n",
      "Episode 1200, Max Tile: 6, Final Score: 616,  Baseline: 980.85, Improvement: -356.35\n",
      "Episode 1250, Max Tile: 6, Final Score: 672,  Baseline: 892.53, Improvement: -211.53\n",
      "Episode 1300, Max Tile: 6, Final Score: 584,  Baseline: 984.29, Improvement: -392.19\n",
      "Episode 1350, Max Tile: 7, Final Score: 1068,  Baseline: 1047.75, Improvement: 31.95\n",
      "Episode 1400, Max Tile: 8, Final Score: 2032,  Baseline: 973.64, Improvement: 1075.36\n",
      "Episode 1450, Max Tile: 6, Final Score: 780,  Baseline: 989.03, Improvement: -198.93\n",
      "Episode 1500, Max Tile: 6, Final Score: 676,  Baseline: 943.01, Improvement: -258.21\n",
      "Episode 1550, Max Tile: 7, Final Score: 1016,  Baseline: 985.44, Improvement: 41.16\n",
      "Episode 1600, Max Tile: 6, Final Score: 692,  Baseline: 1107.22, Improvement: -406.12\n",
      "Episode 1650, Max Tile: 6, Final Score: 716,  Baseline: 968.18, Improvement: -242.78\n",
      "Episode 1700, Max Tile: 7, Final Score: 2272,  Baseline: 1146.61, Improvement: 1146.89\n",
      "Episode 1750, Max Tile: 7, Final Score: 1136,  Baseline: 1110.25, Improvement: 37.95\n",
      "Episode 1800, Max Tile: 5, Final Score: 404,  Baseline: 1034.87, Improvement: -623.87\n",
      "Episode 1850, Max Tile: 6, Final Score: 440,  Baseline: 1013.68, Improvement: -567.08\n",
      "Episode 1900, Max Tile: 8, Final Score: 2400,  Baseline: 1100.09, Improvement: 1320.11\n",
      "Episode 1950, Max Tile: 6, Final Score: 560,  Baseline: 1156.04, Improvement: -588.14\n",
      "Episode 2000, Max Tile: 7, Final Score: 1556,  Baseline: 1215.78, Improvement: 356.02\n",
      "Episode 2050, Max Tile: 7, Final Score: 1536,  Baseline: 1194.39, Improvement: 357.71\n",
      "Episode 2100, Max Tile: 7, Final Score: 1304,  Baseline: 1134.85, Improvement: 182.35\n",
      "Episode 2150, Max Tile: 7, Final Score: 1132,  Baseline: 1022.68, Improvement: 121.12\n",
      "Episode 2200, Max Tile: 5, Final Score: 244,  Baseline: 920.99, Improvement: -672.09\n",
      "Episode 2250, Max Tile: 6, Final Score: 1360,  Baseline: 996.06, Improvement: 378.54\n",
      "Episode 2300, Max Tile: 5, Final Score: 272,  Baseline: 1024.92, Improvement: -747.82\n",
      "Episode 2350, Max Tile: 7, Final Score: 1292,  Baseline: 1224.14, Improvement: 80.76\n",
      "Episode 2400, Max Tile: 6, Final Score: 1212,  Baseline: 1113.41, Improvement: 112.19\n",
      "Episode 2450, Max Tile: 6, Final Score: 812,  Baseline: 1081.36, Improvement: -258.96\n",
      "Episode 2500, Max Tile: 6, Final Score: 588,  Baseline: 1068.84, Improvement: -472.74\n",
      "Episode 2550, Max Tile: 7, Final Score: 1312,  Baseline: 1018.59, Improvement: 307.11\n",
      "Episode 2600, Max Tile: 7, Final Score: 1308,  Baseline: 1161.41, Improvement: 160.19\n",
      "Episode 2650, Max Tile: 6, Final Score: 732,  Baseline: 1111.46, Improvement: -369.56\n",
      "Episode 2700, Max Tile: 7, Final Score: 1188,  Baseline: 1283.36, Improvement: -82.26\n",
      "Episode 2750, Max Tile: 7, Final Score: 980,  Baseline: 1196.90, Improvement: -206.40\n",
      "Episode 2800, Max Tile: 7, Final Score: 1348,  Baseline: 1193.91, Improvement: 167.99\n",
      "Episode 2850, Max Tile: 6, Final Score: 724,  Baseline: 1130.82, Improvement: -397.22\n",
      "Episode 2900, Max Tile: 7, Final Score: 896,  Baseline: 1149.46, Improvement: -243.96\n",
      "Episode 2950, Max Tile: 5, Final Score: 408,  Baseline: 958.75, Improvement: -543.95\n",
      "Episode 3000, Max Tile: 7, Final Score: 1300,  Baseline: 1094.68, Improvement: 219.02\n",
      "Episode 3050, Max Tile: 7, Final Score: 980,  Baseline: 1105.99, Improvement: -115.59\n",
      "Episode 3100, Max Tile: 6, Final Score: 640,  Baseline: 1181.48, Improvement: -532.58\n",
      "Episode 3150, Max Tile: 6, Final Score: 1136,  Baseline: 1028.10, Improvement: 120.50\n",
      "Episode 3200, Max Tile: 7, Final Score: 1408,  Baseline: 1196.40, Improvement: 226.20\n",
      "Episode 3250, Max Tile: 6, Final Score: 620,  Baseline: 1187.48, Improvement: -558.98\n",
      "Episode 3300, Max Tile: 6, Final Score: 824,  Baseline: 1080.78, Improvement: -246.48\n",
      "Episode 3350, Max Tile: 5, Final Score: 320,  Baseline: 955.82, Improvement: -629.92\n",
      "Episode 3400, Max Tile: 6, Final Score: 892,  Baseline: 1224.88, Improvement: -321.98\n",
      "Episode 3450, Max Tile: 6, Final Score: 1100,  Baseline: 1161.03, Improvement: -47.23\n",
      "Episode 3500, Max Tile: 7, Final Score: 980,  Baseline: 1014.87, Improvement: -24.87\n",
      "Episode 3550, Max Tile: 7, Final Score: 1280,  Baseline: 1060.77, Improvement: 232.33\n",
      "Episode 3600, Max Tile: 7, Final Score: 1276,  Baseline: 1216.67, Improvement: 72.43\n",
      "Episode 3650, Max Tile: 6, Final Score: 656,  Baseline: 1100.49, Improvement: -435.29\n",
      "Episode 3700, Max Tile: 7, Final Score: 1116,  Baseline: 1201.89, Improvement: -74.09\n",
      "Episode 3750, Max Tile: 7, Final Score: 1204,  Baseline: 1159.68, Improvement: 56.72\n",
      "Episode 3800, Max Tile: 6, Final Score: 1112,  Baseline: 1164.51, Improvement: -40.41\n",
      "Episode 3850, Max Tile: 8, Final Score: 2884,  Baseline: 1264.04, Improvement: 1643.66\n",
      "Episode 3900, Max Tile: 6, Final Score: 1048,  Baseline: 1133.74, Improvement: -73.04\n",
      "Episode 3950, Max Tile: 6, Final Score: 704,  Baseline: 1177.29, Improvement: -463.89\n",
      "Episode 4000, Max Tile: 7, Final Score: 1228,  Baseline: 1083.28, Improvement: 157.32\n",
      "Episode 4050, Max Tile: 7, Final Score: 1504,  Baseline: 1162.21, Improvement: 357.09\n",
      "Episode 4100, Max Tile: 5, Final Score: 464,  Baseline: 988.68, Improvement: -517.68\n",
      "Episode 4150, Max Tile: 7, Final Score: 1492,  Baseline: 1264.49, Improvement: 242.81\n",
      "Episode 4200, Max Tile: 7, Final Score: 1304,  Baseline: 1126.37, Improvement: 191.33\n",
      "Episode 4250, Max Tile: 5, Final Score: 584,  Baseline: 1048.53, Improvement: -455.73\n",
      "Episode 4300, Max Tile: 7, Final Score: 1296,  Baseline: 1149.85, Improvement: 159.35\n",
      "Episode 4350, Max Tile: 7, Final Score: 1216,  Baseline: 1094.30, Improvement: 134.70\n",
      "Episode 4400, Max Tile: 6, Final Score: 556,  Baseline: 1151.08, Improvement: -587.58\n",
      "Episode 4450, Max Tile: 6, Final Score: 640,  Baseline: 1204.14, Improvement: -555.14\n",
      "Episode 4500, Max Tile: 7, Final Score: 1732,  Baseline: 1246.84, Improvement: 502.46\n",
      "Episode 4550, Max Tile: 7, Final Score: 1052,  Baseline: 1168.55, Improvement: -105.45\n",
      "Episode 4600, Max Tile: 7, Final Score: 1352,  Baseline: 1244.76, Improvement: 121.14\n",
      "Episode 4650, Max Tile: 6, Final Score: 632,  Baseline: 1318.99, Improvement: -678.79\n",
      "Episode 4700, Max Tile: 7, Final Score: 1408,  Baseline: 1135.50, Improvement: 287.00\n",
      "Episode 4750, Max Tile: 8, Final Score: 2764,  Baseline: 1218.23, Improvement: 1568.27\n",
      "Episode 4800, Max Tile: 6, Final Score: 440,  Baseline: 1206.54, Improvement: -760.04\n",
      "Episode 4850, Max Tile: 6, Final Score: 840,  Baseline: 1179.01, Improvement: -328.41\n",
      "Episode 4900, Max Tile: 7, Final Score: 2064,  Baseline: 1328.14, Improvement: 755.06\n",
      "Episode 4950, Max Tile: 7, Final Score: 1376,  Baseline: 1189.14, Improvement: 201.36\n",
      "Episode 5000, Max Tile: 7, Final Score: 1384,  Baseline: 1141.74, Improvement: 256.86\n",
      "Episode 5050, Max Tile: 7, Final Score: 1648,  Baseline: 1192.12, Improvement: 472.38\n",
      "Episode 5100, Max Tile: 5, Final Score: 476,  Baseline: 1089.92, Improvement: -606.52\n",
      "Episode 5150, Max Tile: 6, Final Score: 660,  Baseline: 1159.04, Improvement: -490.34\n",
      "Episode 5200, Max Tile: 8, Final Score: 3168,  Baseline: 1332.57, Improvement: 1861.43\n",
      "Episode 5250, Max Tile: 7, Final Score: 1200,  Baseline: 1089.06, Improvement: 123.84\n",
      "Episode 5300, Max Tile: 6, Final Score: 664,  Baseline: 1221.31, Improvement: -548.51\n",
      "Episode 5350, Max Tile: 6, Final Score: 644,  Baseline: 1192.75, Improvement: -540.25\n",
      "Episode 5400, Max Tile: 7, Final Score: 1732,  Baseline: 1360.85, Improvement: 387.75\n",
      "Episode 5450, Max Tile: 7, Final Score: 1480,  Baseline: 1354.71, Improvement: 140.39\n",
      "Episode 5500, Max Tile: 7, Final Score: 1216,  Baseline: 1142.41, Improvement: 86.19\n",
      "Episode 5550, Max Tile: 7, Final Score: 1084,  Baseline: 1134.50, Improvement: -38.40\n",
      "Episode 5600, Max Tile: 7, Final Score: 1452,  Baseline: 1180.28, Improvement: 286.82\n",
      "Episode 5650, Max Tile: 7, Final Score: 1252,  Baseline: 1328.65, Improvement: -63.35\n",
      "Episode 5700, Max Tile: 6, Final Score: 680,  Baseline: 1181.91, Improvement: -492.41\n",
      "Episode 5750, Max Tile: 5, Final Score: 332,  Baseline: 1187.90, Improvement: -849.90\n",
      "Episode 5800, Max Tile: 6, Final Score: 556,  Baseline: 1271.70, Improvement: -707.90\n",
      "Episode 5850, Max Tile: 8, Final Score: 2664,  Baseline: 1328.56, Improvement: 1356.94\n",
      "Episode 5900, Max Tile: 6, Final Score: 604,  Baseline: 1155.15, Improvement: -542.95\n",
      "Episode 5950, Max Tile: 6, Final Score: 528,  Baseline: 1190.86, Improvement: -655.86\n",
      "Episode 6000, Max Tile: 7, Final Score: 1976,  Baseline: 1355.33, Improvement: 638.97\n",
      "Episode 6050, Max Tile: 7, Final Score: 2036,  Baseline: 1332.89, Improvement: 722.11\n",
      "Episode 6100, Max Tile: 6, Final Score: 1008,  Baseline: 1262.87, Improvement: -242.77\n",
      "Episode 6150, Max Tile: 6, Final Score: 824,  Baseline: 1179.81, Improvement: -346.31\n",
      "Episode 6200, Max Tile: 6, Final Score: 748,  Baseline: 1089.48, Improvement: -331.38\n",
      "Episode 6250, Max Tile: 7, Final Score: 1804,  Baseline: 1402.78, Improvement: 418.82\n",
      "Episode 6300, Max Tile: 7, Final Score: 1172,  Baseline: 1239.56, Improvement: -54.96\n",
      "Episode 6350, Max Tile: 7, Final Score: 1224,  Baseline: 1293.07, Improvement: -55.67\n",
      "Episode 6400, Max Tile: 7, Final Score: 1652,  Baseline: 1325.51, Improvement: 343.09\n",
      "Episode 6450, Max Tile: 6, Final Score: 736,  Baseline: 1284.77, Improvement: -539.07\n",
      "Episode 6500, Max Tile: 7, Final Score: 1008,  Baseline: 1274.04, Improvement: -255.14\n",
      "Episode 6550, Max Tile: 7, Final Score: 1380,  Baseline: 1451.67, Improvement: -56.97\n",
      "Episode 6600, Max Tile: 7, Final Score: 1056,  Baseline: 1273.16, Improvement: -206.06\n",
      "Episode 6650, Max Tile: 7, Final Score: 1520,  Baseline: 1410.33, Improvement: 125.47\n",
      "Episode 6700, Max Tile: 7, Final Score: 1872,  Baseline: 1480.85, Improvement: 408.85\n",
      "Episode 6750, Max Tile: 8, Final Score: 2428,  Baseline: 1474.83, Improvement: 973.67\n",
      "Episode 6800, Max Tile: 7, Final Score: 1956,  Baseline: 1486.78, Improvement: 486.92\n",
      "Episode 6850, Max Tile: 6, Final Score: 640,  Baseline: 1328.25, Improvement: -679.45\n",
      "Episode 6900, Max Tile: 7, Final Score: 1456,  Baseline: 1279.81, Improvement: 191.39\n",
      "Episode 6950, Max Tile: 7, Final Score: 2000,  Baseline: 1344.96, Improvement: 674.04\n",
      "Episode 7000, Max Tile: 7, Final Score: 1292,  Baseline: 1460.78, Improvement: -155.48\n",
      "Episode 7050, Max Tile: 6, Final Score: 1184,  Baseline: 1278.78, Improvement: -80.48\n",
      "Episode 7100, Max Tile: 6, Final Score: 936,  Baseline: 1241.98, Improvement: -294.58\n",
      "Episode 7150, Max Tile: 8, Final Score: 2208,  Baseline: 1550.83, Improvement: 675.97\n",
      "Episode 7200, Max Tile: 6, Final Score: 692,  Baseline: 1311.42, Improvement: -610.22\n",
      "Episode 7250, Max Tile: 7, Final Score: 1412,  Baseline: 1571.01, Improvement: -144.71\n",
      "Episode 7300, Max Tile: 7, Final Score: 1288,  Baseline: 1270.73, Improvement: 30.47\n",
      "Episode 7350, Max Tile: 7, Final Score: 1536,  Baseline: 1337.18, Improvement: 214.82\n",
      "Episode 7400, Max Tile: 8, Final Score: 2412,  Baseline: 1509.01, Improvement: 923.79\n",
      "Episode 7450, Max Tile: 8, Final Score: 2436,  Baseline: 1450.33, Improvement: 1006.37\n",
      "Episode 7500, Max Tile: 7, Final Score: 1516,  Baseline: 1352.42, Improvement: 179.58\n",
      "Episode 7550, Max Tile: 5, Final Score: 468,  Baseline: 1223.68, Improvement: -748.18\n",
      "Episode 7600, Max Tile: 8, Final Score: 2900,  Baseline: 1444.78, Improvement: 1478.92\n",
      "Episode 7650, Max Tile: 7, Final Score: 1516,  Baseline: 1426.44, Improvement: 105.16\n",
      "Episode 7700, Max Tile: 6, Final Score: 836,  Baseline: 1457.61, Improvement: -611.01\n",
      "Episode 7750, Max Tile: 7, Final Score: 1468,  Baseline: 1308.86, Improvement: 174.74\n",
      "Episode 7800, Max Tile: 7, Final Score: 1192,  Baseline: 1266.82, Improvement: -62.62\n",
      "Episode 7850, Max Tile: 7, Final Score: 1416,  Baseline: 1338.63, Improvement: 92.27\n",
      "Episode 7900, Max Tile: 7, Final Score: 1240,  Baseline: 1404.22, Improvement: -151.42\n",
      "Episode 7950, Max Tile: 6, Final Score: 556,  Baseline: 1278.64, Improvement: -714.84\n",
      "Episode 8000, Max Tile: 8, Final Score: 2024,  Baseline: 1232.54, Improvement: 808.36\n",
      "Episode 8050, Max Tile: 9, Final Score: 4804,  Baseline: 1759.10, Improvement: 3078.30\n",
      "Episode 8100, Max Tile: 7, Final Score: 1360,  Baseline: 1409.35, Improvement: -34.85\n",
      "Episode 8150, Max Tile: 8, Final Score: 3184,  Baseline: 1448.39, Improvement: 1761.91\n",
      "Episode 8200, Max Tile: 6, Final Score: 820,  Baseline: 1396.84, Improvement: -566.24\n",
      "Episode 8250, Max Tile: 7, Final Score: 1372,  Baseline: 1485.89, Improvement: -99.59\n",
      "Episode 8300, Max Tile: 5, Final Score: 596,  Baseline: 1349.35, Improvement: -745.05\n",
      "Episode 8350, Max Tile: 7, Final Score: 1116,  Baseline: 1330.91, Improvement: -203.01\n",
      "Episode 8400, Max Tile: 7, Final Score: 1664,  Baseline: 1374.96, Improvement: 304.54\n",
      "Episode 8450, Max Tile: 6, Final Score: 580,  Baseline: 1474.62, Improvement: -886.52\n",
      "Episode 8500, Max Tile: 7, Final Score: 1084,  Baseline: 1525.06, Improvement: -429.96\n",
      "Episode 8550, Max Tile: 7, Final Score: 1648,  Baseline: 1395.34, Improvement: 268.06\n",
      "Episode 8600, Max Tile: 7, Final Score: 1528,  Baseline: 1597.79, Improvement: -53.99\n",
      "Episode 8650, Max Tile: 7, Final Score: 1876,  Baseline: 1495.50, Improvement: 398.60\n",
      "Episode 8700, Max Tile: 6, Final Score: 784,  Baseline: 1442.38, Improvement: -647.68\n",
      "Episode 8750, Max Tile: 8, Final Score: 2564,  Baseline: 1614.42, Improvement: 971.68\n",
      "Episode 8800, Max Tile: 7, Final Score: 1224,  Baseline: 1660.32, Improvement: -423.22\n",
      "Episode 8850, Max Tile: 7, Final Score: 1444,  Baseline: 1476.12, Improvement: -16.62\n",
      "Episode 8900, Max Tile: 8, Final Score: 2684,  Baseline: 1598.87, Improvement: 1106.93\n",
      "Episode 8950, Max Tile: 7, Final Score: 1328,  Baseline: 1705.05, Improvement: -363.45\n",
      "Episode 9000, Max Tile: 7, Final Score: 1396,  Baseline: 1596.52, Improvement: -185.62\n",
      "Episode 9050, Max Tile: 7, Final Score: 1308,  Baseline: 1311.98, Improvement: 9.32\n",
      "Episode 9100, Max Tile: 7, Final Score: 1412,  Baseline: 1694.81, Improvement: -267.91\n",
      "Episode 9150, Max Tile: 7, Final Score: 1364,  Baseline: 1383.81, Improvement: -5.81\n",
      "Episode 9200, Max Tile: 8, Final Score: 2108,  Baseline: 1507.54, Improvement: 618.36\n",
      "Episode 9250, Max Tile: 6, Final Score: 792,  Baseline: 1515.38, Improvement: -712.98\n",
      "Episode 9300, Max Tile: 8, Final Score: 2744,  Baseline: 1701.51, Improvement: 1064.49\n",
      "Episode 9350, Max Tile: 8, Final Score: 2716,  Baseline: 1551.23, Improvement: 1186.57\n",
      "Episode 9400, Max Tile: 6, Final Score: 648,  Baseline: 1589.23, Improvement: -932.63\n",
      "Episode 9450, Max Tile: 8, Final Score: 3112,  Baseline: 1747.21, Improvement: 1390.59\n",
      "Episode 9500, Max Tile: 7, Final Score: 1440,  Baseline: 1479.71, Improvement: -24.51\n",
      "Episode 9550, Max Tile: 7, Final Score: 1100,  Baseline: 1623.87, Improvement: -512.47\n",
      "Episode 9600, Max Tile: 6, Final Score: 832,  Baseline: 1571.54, Improvement: -729.14\n",
      "Episode 9650, Max Tile: 7, Final Score: 2016,  Baseline: 1490.61, Improvement: 544.99\n",
      "Episode 9700, Max Tile: 7, Final Score: 1580,  Baseline: 1599.50, Improvement: -3.10\n",
      "Episode 9750, Max Tile: 8, Final Score: 3564,  Baseline: 1841.88, Improvement: 1752.02\n",
      "Episode 9800, Max Tile: 6, Final Score: 900,  Baseline: 1547.90, Improvement: -636.50\n",
      "Episode 9850, Max Tile: 7, Final Score: 1724,  Baseline: 1746.81, Improvement: -5.21\n",
      "Episode 9900, Max Tile: 7, Final Score: 1176,  Baseline: 1580.08, Improvement: -391.08\n",
      "Episode 9950, Max Tile: 7, Final Score: 1916,  Baseline: 1627.81, Improvement: 306.99\n",
      "Episode 10000, Max Tile: 8, Final Score: 2736,  Baseline: 1780.42, Improvement: 979.48\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "model.eval()\n",
    "env = Game2048Env()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_episodes = 10000\n",
    "baseline = 1500\n",
    "batch_size = 25\n",
    "batch_log_probs = []\n",
    "batch_improvs = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    valid_log_probs = []\n",
    "    finalscore = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "        logits = model(state_tensor)\n",
    "\n",
    "        ranked_actions = torch.argsort(logits, dim=1, descending=True)[0]   #sort by how liikely move is\n",
    "\n",
    "        original_board = env.board.copy()\n",
    "        final_action = None\n",
    "        selected_log_prob = None\n",
    "\n",
    "        movecount = 0   #tracks if the first move was valid \n",
    "        for action in ranked_actions:\n",
    "            test_board, _ = move(original_board.copy(), action.item())\n",
    "            if not np.array_equal(test_board, original_board):\n",
    "                final_action = action.item()\n",
    "                dist = Categorical(logits=logits)\n",
    "                selected_log_prob = dist.log_prob(action)\n",
    "                break\n",
    "            movecount += 1\n",
    "\n",
    "        if final_action is None:    #game is stuck, skip (shouldn't happen)\n",
    "            print(\"SOMETHING WRONG AAAAAUUEEUAGHGEUGHHH\")\n",
    "            break\n",
    "\n",
    "        state, score, done = env.step(final_action)\n",
    "        valid_log_probs.append(selected_log_prob)\n",
    "        finalscore += score\n",
    "\n",
    "    # use baseline to force games to improve\n",
    "    baseline = 0.95 * baseline + 0.05 * finalscore\n",
    "    improvement= finalscore - baseline + 0.1 * len(valid_log_probs)\n",
    "\n",
    "    #use batches for more stable training\n",
    "    batch_log_probs.extend(valid_log_probs)\n",
    "    batch_improvs.extend([improvement] * len(valid_log_probs))\n",
    "\n",
    "    if (episode+1) % batch_size == 0:\n",
    "        loss = 0\n",
    "        for log_prob, improvement in zip(batch_log_probs, batch_improvs):\n",
    "            loss -= log_prob * improvement\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_log_probs = []\n",
    "        batch_improvs = []\n",
    "\n",
    "    if (episode+1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1}, Max Tile: {env.board.max()}, Final Score: {finalscore},  Baseline: {baseline:.2f}, Improvement: {improvement:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_fine.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "the package versions need to be really specific or else it crashes and burns\n",
    "\n",
    "make a new virtual env with the following:\\\n",
    "`python3.9 -m venv tfenv`\n",
    "\n",
    "run this bash script to convert to Tensorflow:\\\n",
    "`pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && onnx-tf convert -i 2048_fine.onnx -o 2048_fine_tf`\n",
    "\n",
    "and then this bash script to convert from Tensorflow to Tensorflow.js:\\\n",
    "`pip install tensorflowjs==4.18.0 && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_fine_tf/ 2048_fine_tfjs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
