{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at making custom TorchRL enviornment for 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from torchrl.envs import GymEnv, EnvBase\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "from torchrl.data import Bounded, Composite, Categorical, Binary\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up,right, down, left\n",
    "\n",
    "def add_tile(board, rng=None):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:  # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_left(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_left(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 2:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_left(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 3:  # left\n",
    "        new_board, reward = move_left(board)\n",
    "    elif direction == 1:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_left(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchRL Env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game2048Env(EnvBase):\n",
    "    def __init__(self, device=\"cpu\", batch_size=None):\n",
    "        #define self.observation_spec, self.action_spec, self.reward_spec here\n",
    "        super().__init__(device=device, batch_size=batch_size)\n",
    "        self.observation_spec = Composite(\n",
    "            observation=Bounded(\n",
    "                low=0.0,\n",
    "                high=18.0,  # max tile is 2^18 in perfect conditions\n",
    "                shape=(BOARD_SIZE, BOARD_SIZE),\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            ),\n",
    "            shape=batch_size\n",
    "        )\n",
    "        self.action_spec = Categorical(\n",
    "            n=len(ACTIONS),\n",
    "            shape=(),\n",
    "            dtype=torch.int64,\n",
    "            device=device\n",
    "        )\n",
    "        self.reward_spec = Bounded(\n",
    "            low=0.0,\n",
    "            high=float('inf'),\n",
    "            shape=(1,),\n",
    "            dtype=torch.float32,\n",
    "            device=device\n",
    "        )\n",
    "        self.done_spec = Composite(\n",
    "            done=Binary(\n",
    "                shape=(1,),\n",
    "                dtype=torch.bool,\n",
    "                device=device\n",
    "            ),\n",
    "            shape=batch_size\n",
    "        )\n",
    "        self.board = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self._set_seed(None)\n",
    "\n",
    "    def _reset(self, tensordict=None):\n",
    "        #returns observation of initial state, reset env to initial state to prepare for next episode\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(self.board, self.rng)\n",
    "        self.board = add_tile(self.board, self.rng)\n",
    "        obs = torch.from_numpy(self.board.copy().astype(np. float32)).to(self.device)\n",
    "        done = torch.zeros(1, dtype=torch.bool, device=self.device)\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"observation\": obs, \n",
    "                \"done\": done\n",
    "            },\n",
    "            batch_size=self.batch_size,\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def _step(self, tensordict):\n",
    "        action = tensordict[\"action\"].item()\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        if not np.array_equal(old_board, self.board):\n",
    "            self.board = add_tile(self.board)\n",
    "        done = is_game_over(self.board)\n",
    "        obs = torch.from_numpy(self.board.copy().astype(np.float32)).to(self.device)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        done_tensor = torch.tensor(done, dtype=torch.bool, device=self.device)\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"observation\": obs,\n",
    "                \"reward\": reward_tensor,\n",
    "                \"done\": done_tensor\n",
    "            },\n",
    "            batch_size=self.batch_size,\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def _set_seed(self, seed):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, 2**31 - 1)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial State:\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Done: False\n",
      "\n",
      "Step 1, Action: 0\n",
      "td params: _StringKeys(dict_keys(['observation', 'done', 'terminated', 'action', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 1. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 2, Action: 1\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 1. 2.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "Reward: 4.0\n",
      "Done: False\n",
      "\n",
      "Step 3, Action: 2\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 1. 2.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 4, Action: 3\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 2. 0.]\n",
      " [2. 2. 0. 0.]]\n",
      "Reward: 4.0\n",
      "Done: False\n",
      "\n",
      "Step 5, Action: 0\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 2. 2. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 6, Action: 1\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 1. 3.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "Reward: 8.0\n",
      "Done: False\n",
      "\n",
      "Step 7, Action: 2\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 3.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 1. 2.]]\n",
      "Reward: 4.0\n",
      "Done: False\n",
      "\n",
      "Step 8, Action: 3\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 0.]\n",
      " [3. 0. 0. 0.]\n",
      " [2. 0. 0. 1.]\n",
      " [1. 2. 0. 0.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 9, Action: 0\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 2. 0. 1.]\n",
      " [3. 0. 0. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [1. 0. 1. 0.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 10, Action: 1\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 1. 2. 1.]\n",
      " [0. 0. 0. 3.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 0. 2.]]\n",
      "Reward: 4.0\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "env = Game2048Env()\n",
    "td = env.reset()\n",
    "print(f\"\\nInitial State:\")\n",
    "print(td[\"observation\"].numpy())\n",
    "print(f\"Done: {td['done'].item()}\")\n",
    "\n",
    "for step in range(10):\n",
    "    action = torch.tensor((step % 4), dtype=torch.int64)\n",
    "    td[\"action\"] = action\n",
    "    print(f\"\\nStep {step + 1}, Action: {action}\")\n",
    "    td = env.step(td)\n",
    "    print(\"td params:\", td.keys())\n",
    "    print(\"next params:\", td[\"next\"].keys())\n",
    "    print(td[\"next\"][\"observation\"].numpy())\n",
    "    print(f\"Reward: {td['next']['reward'].item()}\")\n",
    "    print(f\"Done: {td['next']['done'].item()}\")\n",
    "    td = env.step_mdp(td)  # move to next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "from torchrl.envs import TransformedEnv\n",
    "from torchrl.envs.transforms import StepCounter, Compose\n",
    "from torchrl.modules import EGreedyModule, QValueModule\n",
    "from torchrl.objectives import DQNLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Conv2d(128, 128, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 2 * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        # obs shape: (batch, 4, 4)\n",
    "        x = obs.unsqueeze(1)  # (batch, 1, 4, 4)\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
