{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at making custom TorchRL enviornment for 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from torchrl.envs import GymEnv, EnvBase\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n",
    "from torchrl.data import Bounded, Composite, Categorical, Binary\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up,right, down, left\n",
    "\n",
    "def add_tile(board, rng=None):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:  # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_left(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_left(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 2:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_left(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 3:  # left\n",
    "        new_board, reward = move_left(board)\n",
    "    elif direction == 1:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_left(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchRL Env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game2048Env(EnvBase):\n",
    "    def __init__(self, device=\"cpu\", batch_size=None):\n",
    "        #define self.observation_spec, self.action_spec, self.reward_spec here\n",
    "        super().__init__(device=device, batch_size=batch_size)\n",
    "        self.observation_spec = Composite(\n",
    "            observation=Bounded(\n",
    "                low=0.0,\n",
    "                high=18.0,  # max tile is 2^18 in perfect conditions\n",
    "                shape=(BOARD_SIZE, BOARD_SIZE),\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            ),\n",
    "            shape=batch_size\n",
    "        )\n",
    "        self.action_spec = Categorical(\n",
    "            n=len(ACTIONS),\n",
    "            shape=(),\n",
    "            dtype=torch.int64,\n",
    "            device=device\n",
    "        )\n",
    "        self.reward_spec = Bounded(\n",
    "            low=0.0,\n",
    "            high=float('inf'),\n",
    "            shape=(1,),\n",
    "            dtype=torch.float32,\n",
    "            device=device\n",
    "        )\n",
    "        self.done_spec = Composite(\n",
    "            done=Binary(\n",
    "                shape=(1,),\n",
    "                dtype=torch.bool,\n",
    "                device=device\n",
    "            ),\n",
    "            shape=batch_size\n",
    "        )\n",
    "        self.board = None\n",
    "        self.rng = np.random.RandomState()\n",
    "        self._set_seed(None)\n",
    "\n",
    "    def _reset(self, tensordict=None):\n",
    "        #returns observation of initial state, reset env to initial state to prepare for next episode\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(self.board, self.rng)\n",
    "        self.board = add_tile(self.board, self.rng)\n",
    "        obs = torch.from_numpy(self.board.copy().astype(np. float32)).to(self.device)\n",
    "        done = torch.zeros(1, dtype=torch.bool, device=self.device)\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"observation\": obs, \n",
    "                \"done\": done\n",
    "            },\n",
    "            batch_size=self.batch_size,\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def _step(self, tensordict):\n",
    "        action = tensordict[\"action\"].item()\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        if not np.array_equal(old_board, self.board):\n",
    "            self.board = add_tile(self.board)\n",
    "        done = is_game_over(self.board)\n",
    "        obs = torch.from_numpy(self.board.copy().astype(np.float32)).to(self.device)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        done_tensor = torch.tensor(done, dtype=torch.bool, device=self.device)\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"observation\": obs,\n",
    "                \"reward\": reward_tensor,\n",
    "                \"done\": done_tensor\n",
    "            },\n",
    "            batch_size=self.batch_size,\n",
    "            device=self.device\n",
    "        )\n",
    "    \n",
    "    def _set_seed(self, seed):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, 2**31 - 1)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial State:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 2.]]\n",
      "Done: False\n",
      "\n",
      "Step 1, Action: 0\n",
      "td params: _StringKeys(dict_keys(['observation', 'done', 'terminated', 'action', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 2.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 2, Action: 1\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 1. 2.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 1.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 3, Action: 2\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 2.]\n",
      " [1. 0. 1. 1.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 4, Action: 3\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [2. 1. 0. 0.]]\n",
      "Reward: 4.0\n",
      "Done: False\n",
      "\n",
      "Step 5, Action: 0\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 2. 0. 0.]\n",
      " [3. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Reward: 12.0\n",
      "Done: False\n",
      "\n",
      "Step 6, Action: 1\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 1. 2.]\n",
      " [0. 0. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 7, Action: 2\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 0. 3.]\n",
      " [0. 1. 1. 1.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 8, Action: 3\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 0. 0. 0.]\n",
      " [2. 0. 0. 1.]\n",
      " [3. 0. 0. 0.]\n",
      " [2. 1. 0. 0.]]\n",
      "Reward: 4.0\n",
      "Done: False\n",
      "\n",
      "Step 9, Action: 0\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[1. 1. 0. 1.]\n",
      " [2. 0. 0. 0.]\n",
      " [3. 2. 0. 0.]\n",
      " [2. 0. 0. 0.]]\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "Step 10, Action: 1\n",
      "td params: _StringKeys(dict_keys(['observation', 'action', 'done', 'terminated', 'next']))\n",
      "next params: _StringKeys(dict_keys(['observation', 'reward', 'done', 'terminated']))\n",
      "[[0. 0. 1. 2.]\n",
      " [0. 0. 0. 2.]\n",
      " [0. 0. 3. 2.]\n",
      " [0. 2. 0. 2.]]\n",
      "Reward: 4.0\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "env = Game2048Env()\n",
    "td = env.reset()\n",
    "print(f\"\\nInitial State:\")\n",
    "print(td[\"observation\"].numpy())\n",
    "print(f\"Done: {td['done'].item()}\")\n",
    "\n",
    "for step in range(10):\n",
    "    action = torch.tensor((step % 4), dtype=torch.int64)\n",
    "    td[\"action\"] = action\n",
    "    print(f\"\\nStep {step + 1}, Action: {action}\")\n",
    "    td = env.step(td)\n",
    "    print(\"td params:\", td.keys())\n",
    "    print(\"next params:\", td[\"next\"].keys())\n",
    "    print(td[\"next\"][\"observation\"].numpy())\n",
    "    print(f\"Reward: {td['next']['reward'].item()}\")\n",
    "    print(f\"Done: {td['next']['done'].item()}\")\n",
    "    td = env.step_mdp(td)  # move to next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
