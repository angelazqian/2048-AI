{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with RL Finetuning through Self-Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for subdir in os.listdir(data_dir):\n",
    "    subdir_path = os.path.join(data_dir, subdir)\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line.strip())\n",
    "                if \"state\" in data and \"action\" in data:\n",
    "                    X.append(data[\"state\"])\n",
    "                    Y.append(data[\"action\"])\n",
    "X = np.array(X)\n",
    "X[X > 0] = np.log2(X[X > 0])    #replace with log2 for simplicity\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=26)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(Y)   #overwrite with full dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NumPy Arrays to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "X shape: torch.Size([64, 1, 4, 4])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):   #reshape to fit CNN input, -1 to auto infer batch size, 1 for single channel\n",
    "        self.X = torch.from_numpy(X.astype(np.float32)).reshape(-1, 1, 4, 4)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "   \n",
    "batch_size = 64\n",
    "\n",
    "#instantiate training and test data\n",
    "train_data = Data(X_train, y_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test, y_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#sanity check\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = 16\n",
    "hidden_dim1 = 256\n",
    "hidden_dim2 = 128\n",
    "output_dim = 4\n",
    "\n",
    "class CNN(nn.Module):   #use CNN because input is image-like (4x4 grid)\n",
    "    def __init__(self, output_dim=4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 128)  # final output size after convs\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNN(output_dim=output_dim)\n",
    "print(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch 2/30\n",
      "Epoch 3/30\n",
      "Epoch 4/30\n",
      "Epoch 5/30\n",
      "Epoch 6/30\n",
      "Epoch 7/30\n",
      "Epoch 8/30\n",
      "Epoch 9/30\n",
      "Epoch 10/30\n",
      "Epoch 11/30\n",
      "Epoch 12/30\n",
      "Epoch 13/30\n",
      "Epoch 14/30\n",
      "Epoch 15/30\n",
      "Epoch 16/30\n",
      "Epoch 17/30\n",
      "Epoch 18/30\n",
      "Epoch 19/30\n",
      "Epoch 20/30\n",
      "Epoch 21/30\n",
      "Epoch 22/30\n",
      "Epoch 23/30\n",
      "Epoch 24/30\n",
      "Epoch 25/30\n",
      "Epoch 26/30\n",
      "Epoch 27/30\n",
      "Epoch 28/30\n",
      "Epoch 29/30\n",
      "Epoch 30/30\n",
      "DONE!!! :3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "\n",
    "print(\"DONE!!! :3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86%\n",
      "Prediction distribution:\n",
      "0:  32574\n",
      "1:  32009\n",
      "2:  31895\n",
      "3:  31937\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "results = [0,0,0,0]\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get model outputs\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_test.extend(y.tolist())\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        for pred in predicted:\n",
    "            results[pred.item()] += 1\n",
    "\n",
    "print(f'Accuracy: {100 * correct // total}%')\n",
    "print(f'Prediction distribution:')\n",
    "print(f'0:  {results[0]}')\n",
    "print(f'1:  {results[1]}')\n",
    "print(f'2:  {results[2]}')\n",
    "print(f'3:  {results[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Imitation to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_imitation.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Imitation ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "the package versions need to be really specific or else it crashes and burns\n",
    "\n",
    "make a new virtual env with the following:\\\n",
    "`python3.9 -m venv tfenv`\n",
    "\n",
    "run this bash script to convert to Tensorflow:\\\n",
    "`pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && onnx-tf convert -i 2048_imitation.onnx -o 2048_imitation_tf`\n",
    "\n",
    "and then this bash script to convert from Tensorflow to Tensorflow.js:\\\n",
    "`pip install tensorflowjs==4.18.0 && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_imitation_tf/ 2048_imitation_tfjs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up, down, left, right\n",
    "\n",
    "def add_tile(board):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:   # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_right(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_right(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 1:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 2:  # left\n",
    "        new_board, reward = move_right(board)\n",
    "    elif direction == 3:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Game2048Env:\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(add_tile(self.board))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # old_max_tile = np.max(self.board)\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        changed = not np.array_equal(self.board, old_board)\n",
    "        if changed: # only add a tile if the board changed\n",
    "            self.board = add_tile(self.board)\n",
    "        # new_max_tile = np.max(self.board)\n",
    "        # reward = (new_max_tile > old_max_tile)  # reward for increasing max tile, small reward for merging\n",
    "        done = is_game_over(self.board)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        board = self.board.copy()\n",
    "        board = np.where(board > 0, board, 0)\n",
    "        board = board.astype(np.float32)\n",
    "        board = board.reshape(1, 1, 4, 4)\n",
    "        return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Max Tile: 6, Final Score: 692,  Baseline: 971.73, Improvement: -270.33\n",
      "Episode 100, Max Tile: 7, Final Score: 1232,  Baseline: 1014.97, Improvement: 230.13\n",
      "Episode 150, Max Tile: 6, Final Score: 960,  Baseline: 858.04, Improvement: 113.36\n",
      "Episode 200, Max Tile: 7, Final Score: 1228,  Baseline: 1063.01, Improvement: 177.39\n",
      "Episode 250, Max Tile: 7, Final Score: 1224,  Baseline: 904.91, Improvement: 331.89\n",
      "Episode 300, Max Tile: 6, Final Score: 780,  Baseline: 961.50, Improvement: -171.40\n",
      "Episode 350, Max Tile: 6, Final Score: 688,  Baseline: 989.18, Improvement: -292.18\n",
      "Episode 400, Max Tile: 6, Final Score: 488,  Baseline: 907.70, Improvement: -413.10\n",
      "Episode 450, Max Tile: 7, Final Score: 940,  Baseline: 1050.63, Improvement: -101.03\n",
      "Episode 500, Max Tile: 6, Final Score: 692,  Baseline: 872.91, Improvement: -171.51\n",
      "Episode 550, Max Tile: 8, Final Score: 2124,  Baseline: 938.05, Improvement: 1203.75\n",
      "Episode 600, Max Tile: 6, Final Score: 784,  Baseline: 1044.82, Improvement: -250.52\n",
      "Episode 650, Max Tile: 6, Final Score: 664,  Baseline: 922.24, Improvement: -249.24\n",
      "Episode 700, Max Tile: 6, Final Score: 752,  Baseline: 829.91, Improvement: -68.41\n",
      "Episode 750, Max Tile: 6, Final Score: 672,  Baseline: 926.41, Improvement: -245.61\n",
      "Episode 800, Max Tile: 7, Final Score: 1488,  Baseline: 1036.85, Improvement: 466.75\n",
      "Episode 850, Max Tile: 7, Final Score: 1280,  Baseline: 1052.83, Improvement: 240.27\n",
      "Episode 900, Max Tile: 7, Final Score: 1004,  Baseline: 944.93, Improvement: 69.77\n",
      "Episode 950, Max Tile: 8, Final Score: 2204,  Baseline: 1140.92, Improvement: 1081.48\n",
      "Episode 1000, Max Tile: 6, Final Score: 588,  Baseline: 1216.87, Improvement: -620.67\n",
      "Episode 1050, Max Tile: 8, Final Score: 2192,  Baseline: 1066.88, Improvement: 1144.02\n",
      "Episode 1100, Max Tile: 7, Final Score: 1168,  Baseline: 1164.53, Improvement: 16.07\n",
      "Episode 1150, Max Tile: 7, Final Score: 920,  Baseline: 988.28, Improvement: -58.28\n",
      "Episode 1200, Max Tile: 7, Final Score: 1036,  Baseline: 1077.95, Improvement: -30.95\n",
      "Episode 1250, Max Tile: 7, Final Score: 1212,  Baseline: 1077.29, Improvement: 147.61\n",
      "Episode 1300, Max Tile: 6, Final Score: 616,  Baseline: 1106.14, Improvement: -481.64\n",
      "Episode 1350, Max Tile: 6, Final Score: 536,  Baseline: 1105.58, Improvement: -562.28\n",
      "Episode 1400, Max Tile: 6, Final Score: 716,  Baseline: 922.76, Improvement: -197.16\n",
      "Episode 1450, Max Tile: 6, Final Score: 972,  Baseline: 1124.59, Improvement: -140.99\n",
      "Episode 1500, Max Tile: 7, Final Score: 1684,  Baseline: 1175.13, Improvement: 524.67\n",
      "Episode 1550, Max Tile: 7, Final Score: 1416,  Baseline: 1019.42, Improvement: 410.98\n",
      "Episode 1600, Max Tile: 7, Final Score: 1080,  Baseline: 1097.47, Improvement: -5.57\n",
      "Episode 1650, Max Tile: 5, Final Score: 392,  Baseline: 1067.53, Improvement: -669.03\n",
      "Episode 1700, Max Tile: 7, Final Score: 1348,  Baseline: 1141.36, Improvement: 220.54\n",
      "Episode 1750, Max Tile: 7, Final Score: 1456,  Baseline: 1145.09, Improvement: 326.11\n",
      "Episode 1800, Max Tile: 7, Final Score: 1392,  Baseline: 1152.76, Improvement: 253.74\n",
      "Episode 1850, Max Tile: 7, Final Score: 1268,  Baseline: 1088.99, Improvement: 192.81\n",
      "Episode 1900, Max Tile: 6, Final Score: 588,  Baseline: 1036.98, Improvement: -440.88\n",
      "Episode 1950, Max Tile: 7, Final Score: 1236,  Baseline: 1059.62, Improvement: 190.08\n",
      "Episode 2000, Max Tile: 8, Final Score: 2936,  Baseline: 1231.05, Improvement: 1728.65\n",
      "Episode 2050, Max Tile: 6, Final Score: 936,  Baseline: 1042.46, Improvement: -94.96\n",
      "Episode 2100, Max Tile: 5, Final Score: 352,  Baseline: 1124.71, Improvement: -766.41\n",
      "Episode 2150, Max Tile: 6, Final Score: 728,  Baseline: 1015.02, Improvement: -276.92\n",
      "Episode 2200, Max Tile: 6, Final Score: 560,  Baseline: 977.31, Improvement: -409.91\n",
      "Episode 2250, Max Tile: 7, Final Score: 1320,  Baseline: 1187.20, Improvement: 147.50\n",
      "Episode 2300, Max Tile: 6, Final Score: 680,  Baseline: 1339.66, Improvement: -650.86\n",
      "Episode 2350, Max Tile: 6, Final Score: 932,  Baseline: 1231.00, Improvement: -287.60\n",
      "Episode 2400, Max Tile: 6, Final Score: 868,  Baseline: 1092.82, Improvement: -214.22\n",
      "Episode 2450, Max Tile: 7, Final Score: 1224,  Baseline: 1210.90, Improvement: 26.20\n",
      "Episode 2500, Max Tile: 6, Final Score: 684,  Baseline: 1221.81, Improvement: -528.91\n",
      "Episode 2550, Max Tile: 7, Final Score: 1640,  Baseline: 1105.30, Improvement: 551.30\n",
      "Episode 2600, Max Tile: 7, Final Score: 1188,  Baseline: 937.46, Improvement: 263.54\n",
      "Episode 2650, Max Tile: 6, Final Score: 708,  Baseline: 1059.91, Improvement: -342.31\n",
      "Episode 2700, Max Tile: 7, Final Score: 1192,  Baseline: 1115.11, Improvement: 88.89\n",
      "Episode 2750, Max Tile: 7, Final Score: 1064,  Baseline: 1196.27, Improvement: -120.67\n",
      "Episode 2800, Max Tile: 6, Final Score: 668,  Baseline: 1219.55, Improvement: -542.55\n",
      "Episode 2850, Max Tile: 7, Final Score: 1296,  Baseline: 1224.57, Improvement: 85.13\n",
      "Episode 2900, Max Tile: 6, Final Score: 592,  Baseline: 1131.05, Improvement: -531.05\n",
      "Episode 2950, Max Tile: 8, Final Score: 3208,  Baseline: 1224.19, Improvement: 2010.11\n",
      "Episode 3000, Max Tile: 8, Final Score: 2380,  Baseline: 1153.83, Improvement: 1246.27\n",
      "Episode 3050, Max Tile: 7, Final Score: 1728,  Baseline: 1209.33, Improvement: 535.97\n",
      "Episode 3100, Max Tile: 7, Final Score: 1340,  Baseline: 1348.53, Improvement: 5.47\n",
      "Episode 3150, Max Tile: 7, Final Score: 1220,  Baseline: 1330.47, Improvement: -97.77\n",
      "Episode 3200, Max Tile: 5, Final Score: 456,  Baseline: 1231.75, Improvement: -768.85\n",
      "Episode 3250, Max Tile: 7, Final Score: 1260,  Baseline: 1164.54, Improvement: 109.16\n",
      "Episode 3300, Max Tile: 7, Final Score: 1024,  Baseline: 1294.52, Improvement: -259.52\n",
      "Episode 3350, Max Tile: 6, Final Score: 584,  Baseline: 1171.87, Improvement: -579.77\n",
      "Episode 3400, Max Tile: 6, Final Score: 884,  Baseline: 1593.93, Improvement: -699.23\n",
      "Episode 3450, Max Tile: 8, Final Score: 2740,  Baseline: 1190.75, Improvement: 1572.65\n",
      "Episode 3500, Max Tile: 7, Final Score: 1640,  Baseline: 1348.46, Improvement: 308.14\n",
      "Episode 3550, Max Tile: 6, Final Score: 752,  Baseline: 1140.45, Improvement: -379.35\n",
      "Episode 3600, Max Tile: 6, Final Score: 712,  Baseline: 1230.98, Improvement: -509.38\n",
      "Episode 3650, Max Tile: 8, Final Score: 1904,  Baseline: 1243.30, Improvement: 676.30\n",
      "Episode 3700, Max Tile: 6, Final Score: 712,  Baseline: 1440.16, Improvement: -718.86\n",
      "Episode 3750, Max Tile: 7, Final Score: 1628,  Baseline: 1191.83, Improvement: 452.57\n",
      "Episode 3800, Max Tile: 6, Final Score: 668,  Baseline: 1225.86, Improvement: -549.06\n",
      "Episode 3850, Max Tile: 6, Final Score: 1008,  Baseline: 1261.87, Improvement: -241.77\n",
      "Episode 3900, Max Tile: 7, Final Score: 1652,  Baseline: 1286.82, Improvement: 382.08\n",
      "Episode 3950, Max Tile: 5, Final Score: 308,  Baseline: 1214.68, Improvement: -901.08\n",
      "Episode 4000, Max Tile: 6, Final Score: 564,  Baseline: 1423.15, Improvement: -851.15\n",
      "Episode 4050, Max Tile: 7, Final Score: 1372,  Baseline: 1448.50, Improvement: -62.10\n",
      "Episode 4100, Max Tile: 7, Final Score: 1028,  Baseline: 1132.23, Improvement: -93.13\n",
      "Episode 4150, Max Tile: 6, Final Score: 660,  Baseline: 1300.27, Improvement: -631.47\n",
      "Episode 4200, Max Tile: 7, Final Score: 1352,  Baseline: 1412.93, Improvement: -46.73\n",
      "Episode 4250, Max Tile: 6, Final Score: 640,  Baseline: 1380.16, Improvement: -731.56\n",
      "Episode 4300, Max Tile: 6, Final Score: 608,  Baseline: 1259.33, Improvement: -643.03\n",
      "Episode 4350, Max Tile: 7, Final Score: 1200,  Baseline: 1450.24, Improvement: -237.64\n",
      "Episode 4400, Max Tile: 7, Final Score: 2068,  Baseline: 1308.39, Improvement: 779.01\n",
      "Episode 4450, Max Tile: 7, Final Score: 1044,  Baseline: 1410.87, Improvement: -355.37\n",
      "Episode 4500, Max Tile: 7, Final Score: 1208,  Baseline: 1165.25, Improvement: 55.55\n",
      "Episode 4550, Max Tile: 7, Final Score: 1644,  Baseline: 1671.37, Improvement: -10.67\n",
      "Episode 4600, Max Tile: 7, Final Score: 1412,  Baseline: 1391.48, Improvement: 34.82\n",
      "Episode 4650, Max Tile: 8, Final Score: 2120,  Baseline: 1380.58, Improvement: 757.02\n",
      "Episode 4700, Max Tile: 6, Final Score: 700,  Baseline: 1306.84, Improvement: -597.54\n",
      "Episode 4750, Max Tile: 6, Final Score: 760,  Baseline: 1489.90, Improvement: -719.70\n",
      "Episode 4800, Max Tile: 5, Final Score: 364,  Baseline: 1553.81, Improvement: -1183.71\n",
      "Episode 4850, Max Tile: 7, Final Score: 1312,  Baseline: 1386.39, Improvement: -60.89\n",
      "Episode 4900, Max Tile: 7, Final Score: 1240,  Baseline: 1552.18, Improvement: -299.28\n",
      "Episode 4950, Max Tile: 7, Final Score: 1204,  Baseline: 1296.05, Improvement: -79.05\n",
      "Episode 5000, Max Tile: 6, Final Score: 736,  Baseline: 1619.46, Improvement: -873.76\n",
      "Episode 5050, Max Tile: 8, Final Score: 2676,  Baseline: 1390.86, Improvement: 1308.04\n",
      "Episode 5100, Max Tile: 8, Final Score: 2384,  Baseline: 1540.67, Improvement: 863.33\n",
      "Episode 5150, Max Tile: 7, Final Score: 1788,  Baseline: 1542.99, Improvement: 261.91\n",
      "Episode 5200, Max Tile: 6, Final Score: 632,  Baseline: 1601.69, Improvement: -961.59\n",
      "Episode 5250, Max Tile: 6, Final Score: 680,  Baseline: 1396.80, Improvement: -707.90\n",
      "Episode 5300, Max Tile: 7, Final Score: 1140,  Baseline: 1421.07, Improvement: -268.77\n",
      "Episode 5350, Max Tile: 7, Final Score: 1328,  Baseline: 1509.38, Improvement: -167.58\n",
      "Episode 5400, Max Tile: 7, Final Score: 1668,  Baseline: 1431.15, Improvement: 253.85\n",
      "Episode 5450, Max Tile: 7, Final Score: 1192,  Baseline: 1336.44, Improvement: -131.44\n",
      "Episode 5500, Max Tile: 7, Final Score: 1660,  Baseline: 1544.22, Improvement: 132.88\n",
      "Episode 5550, Max Tile: 6, Final Score: 648,  Baseline: 1390.60, Improvement: -734.00\n",
      "Episode 5600, Max Tile: 8, Final Score: 2680,  Baseline: 1313.03, Improvement: 1388.67\n",
      "Episode 5650, Max Tile: 8, Final Score: 3060,  Baseline: 1492.26, Improvement: 1592.74\n",
      "Episode 5700, Max Tile: 6, Final Score: 748,  Baseline: 1428.61, Improvement: -671.21\n",
      "Episode 5750, Max Tile: 7, Final Score: 1344,  Baseline: 1467.12, Improvement: -109.12\n",
      "Episode 5800, Max Tile: 7, Final Score: 1396,  Baseline: 1510.73, Improvement: -100.43\n",
      "Episode 5850, Max Tile: 7, Final Score: 1484,  Baseline: 1532.98, Improvement: -33.38\n",
      "Episode 5900, Max Tile: 7, Final Score: 1380,  Baseline: 1722.78, Improvement: -328.88\n",
      "Episode 5950, Max Tile: 7, Final Score: 1380,  Baseline: 1695.43, Improvement: -301.23\n",
      "Episode 6000, Max Tile: 7, Final Score: 1532,  Baseline: 1344.78, Improvement: 203.52\n",
      "Episode 6050, Max Tile: 6, Final Score: 796,  Baseline: 1338.50, Improvement: -532.80\n",
      "Episode 6100, Max Tile: 7, Final Score: 1644,  Baseline: 1413.13, Improvement: 247.27\n",
      "Episode 6150, Max Tile: 8, Final Score: 2184,  Baseline: 1668.18, Improvement: 533.62\n",
      "Episode 6200, Max Tile: 7, Final Score: 1276,  Baseline: 1673.72, Improvement: -384.52\n",
      "Episode 6250, Max Tile: 7, Final Score: 1696,  Baseline: 1681.03, Improvement: 31.87\n",
      "Episode 6300, Max Tile: 8, Final Score: 2320,  Baseline: 1715.63, Improvement: 623.77\n",
      "Episode 6350, Max Tile: 7, Final Score: 2108,  Baseline: 1655.99, Improvement: 471.61\n",
      "Episode 6400, Max Tile: 6, Final Score: 532,  Baseline: 1600.56, Improvement: -1061.26\n",
      "Episode 6450, Max Tile: 7, Final Score: 1428,  Baseline: 1643.80, Improvement: -202.20\n",
      "Episode 6500, Max Tile: 7, Final Score: 1300,  Baseline: 1476.76, Improvement: -163.56\n",
      "Episode 6550, Max Tile: 5, Final Score: 232,  Baseline: 1351.26, Improvement: -1114.36\n",
      "Episode 6600, Max Tile: 8, Final Score: 2076,  Baseline: 1450.59, Improvement: 642.41\n",
      "Episode 6650, Max Tile: 7, Final Score: 1216,  Baseline: 1875.38, Improvement: -646.68\n",
      "Episode 6700, Max Tile: 8, Final Score: 2352,  Baseline: 1714.40, Improvement: 657.40\n",
      "Episode 6750, Max Tile: 6, Final Score: 1072,  Baseline: 1529.27, Improvement: -444.67\n",
      "Episode 6800, Max Tile: 8, Final Score: 2584,  Baseline: 1418.63, Improvement: 1187.07\n",
      "Episode 6850, Max Tile: 7, Final Score: 1716,  Baseline: 1461.07, Improvement: 272.33\n",
      "Episode 6900, Max Tile: 6, Final Score: 568,  Baseline: 1405.71, Improvement: -829.41\n",
      "Episode 6950, Max Tile: 8, Final Score: 2268,  Baseline: 1672.52, Improvement: 614.98\n",
      "Episode 7000, Max Tile: 7, Final Score: 1128,  Baseline: 1864.17, Improvement: -723.87\n",
      "Episode 7050, Max Tile: 7, Final Score: 2080,  Baseline: 1834.63, Improvement: 264.67\n",
      "Episode 7100, Max Tile: 7, Final Score: 1368,  Baseline: 1592.73, Improvement: -210.43\n",
      "Episode 7150, Max Tile: 7, Final Score: 1612,  Baseline: 1672.36, Improvement: -44.26\n",
      "Episode 7200, Max Tile: 7, Final Score: 1352,  Baseline: 1501.64, Improvement: -135.64\n",
      "Episode 7250, Max Tile: 7, Final Score: 1328,  Baseline: 1625.73, Improvement: -283.93\n",
      "Episode 7300, Max Tile: 7, Final Score: 1876,  Baseline: 1549.24, Improvement: 344.66\n",
      "Episode 7350, Max Tile: 7, Final Score: 1384,  Baseline: 1558.76, Improvement: -160.86\n",
      "Episode 7400, Max Tile: 6, Final Score: 924,  Baseline: 1394.77, Improvement: -459.57\n",
      "Episode 7450, Max Tile: 7, Final Score: 2304,  Baseline: 1426.70, Improvement: 899.30\n",
      "Episode 7500, Max Tile: 8, Final Score: 2336,  Baseline: 1600.93, Improvement: 754.77\n",
      "Episode 7550, Max Tile: 7, Final Score: 884,  Baseline: 1636.55, Improvement: -743.25\n",
      "Episode 7600, Max Tile: 8, Final Score: 2372,  Baseline: 1707.61, Improvement: 684.29\n",
      "Episode 7650, Max Tile: 7, Final Score: 1716,  Baseline: 1492.23, Improvement: 239.67\n",
      "Episode 7700, Max Tile: 7, Final Score: 1540,  Baseline: 1606.48, Improvement: -50.68\n",
      "Episode 7750, Max Tile: 7, Final Score: 1332,  Baseline: 1660.05, Improvement: -314.45\n",
      "Episode 7800, Max Tile: 6, Final Score: 920,  Baseline: 1928.15, Improvement: -997.05\n",
      "Episode 7850, Max Tile: 7, Final Score: 1556,  Baseline: 1780.45, Improvement: -208.75\n",
      "Episode 7900, Max Tile: 6, Final Score: 700,  Baseline: 1814.66, Improvement: -1105.46\n",
      "Episode 7950, Max Tile: 7, Final Score: 1596,  Baseline: 1717.26, Improvement: -104.86\n",
      "Episode 8000, Max Tile: 7, Final Score: 1084,  Baseline: 1639.20, Improvement: -543.80\n",
      "Episode 8050, Max Tile: 8, Final Score: 2852,  Baseline: 1654.38, Improvement: 1220.72\n",
      "Episode 8100, Max Tile: 7, Final Score: 1236,  Baseline: 1780.48, Improvement: -531.38\n",
      "Episode 8150, Max Tile: 6, Final Score: 904,  Baseline: 1620.80, Improvement: -705.70\n",
      "Episode 8200, Max Tile: 8, Final Score: 2632,  Baseline: 1685.67, Improvement: 968.63\n",
      "Episode 8250, Max Tile: 8, Final Score: 2592,  Baseline: 1552.60, Improvement: 1062.10\n",
      "Episode 8300, Max Tile: 7, Final Score: 1452,  Baseline: 1538.94, Improvement: -71.74\n",
      "Episode 8350, Max Tile: 6, Final Score: 628,  Baseline: 1636.12, Improvement: -999.52\n",
      "Episode 8400, Max Tile: 8, Final Score: 2440,  Baseline: 1548.11, Improvement: 912.19\n",
      "Episode 8450, Max Tile: 7, Final Score: 1720,  Baseline: 1490.88, Improvement: 246.62\n",
      "Episode 8500, Max Tile: 8, Final Score: 2328,  Baseline: 1801.75, Improvement: 545.55\n",
      "Episode 8550, Max Tile: 7, Final Score: 1544,  Baseline: 1638.89, Improvement: -79.29\n",
      "Episode 8600, Max Tile: 6, Final Score: 940,  Baseline: 1737.28, Improvement: -785.68\n",
      "Episode 8650, Max Tile: 7, Final Score: 2608,  Baseline: 1978.81, Improvement: 652.69\n",
      "Episode 8700, Max Tile: 7, Final Score: 1616,  Baseline: 1855.45, Improvement: -223.25\n",
      "Episode 8750, Max Tile: 8, Final Score: 3604,  Baseline: 1727.24, Improvement: 1905.06\n",
      "Episode 8800, Max Tile: 7, Final Score: 1456,  Baseline: 1592.20, Improvement: -121.10\n",
      "Episode 8850, Max Tile: 8, Final Score: 2884,  Baseline: 1670.66, Improvement: 1237.14\n",
      "Episode 8900, Max Tile: 6, Final Score: 704,  Baseline: 1805.81, Improvement: -1092.41\n",
      "Episode 8950, Max Tile: 7, Final Score: 2188,  Baseline: 1757.42, Improvement: 451.28\n",
      "Episode 9000, Max Tile: 6, Final Score: 744,  Baseline: 1466.34, Improvement: -712.44\n",
      "Episode 9050, Max Tile: 7, Final Score: 1764,  Baseline: 1802.64, Improvement: -20.54\n",
      "Episode 9100, Max Tile: 8, Final Score: 3176,  Baseline: 1862.68, Improvement: 1339.52\n",
      "Episode 9150, Max Tile: 7, Final Score: 2020,  Baseline: 1788.83, Improvement: 249.57\n",
      "Episode 9200, Max Tile: 7, Final Score: 1384,  Baseline: 1836.89, Improvement: -438.29\n",
      "Episode 9250, Max Tile: 7, Final Score: 1500,  Baseline: 1650.68, Improvement: -135.38\n",
      "Episode 9300, Max Tile: 8, Final Score: 2316,  Baseline: 1792.48, Improvement: 543.82\n",
      "Episode 9350, Max Tile: 8, Final Score: 2600,  Baseline: 2086.40, Improvement: 536.20\n",
      "Episode 9400, Max Tile: 7, Final Score: 1300,  Baseline: 1761.17, Improvement: -447.97\n",
      "Episode 9450, Max Tile: 8, Final Score: 2644,  Baseline: 1782.13, Improvement: 884.37\n",
      "Episode 9500, Max Tile: 9, Final Score: 5160,  Baseline: 1815.05, Improvement: 3380.75\n",
      "Episode 9550, Max Tile: 8, Final Score: 3284,  Baseline: 1798.33, Improvement: 1512.57\n",
      "Episode 9600, Max Tile: 7, Final Score: 1480,  Baseline: 1618.73, Improvement: -122.83\n",
      "Episode 9650, Max Tile: 7, Final Score: 1436,  Baseline: 1718.92, Improvement: -268.12\n",
      "Episode 9700, Max Tile: 6, Final Score: 656,  Baseline: 1901.71, Improvement: -1237.11\n",
      "Episode 9750, Max Tile: 8, Final Score: 2528,  Baseline: 1718.27, Improvement: 831.43\n",
      "Episode 9800, Max Tile: 7, Final Score: 1568,  Baseline: 1836.91, Improvement: -252.51\n",
      "Episode 9850, Max Tile: 8, Final Score: 2300,  Baseline: 1535.90, Improvement: 783.10\n",
      "Episode 9900, Max Tile: 7, Final Score: 2160,  Baseline: 1647.37, Improvement: 532.73\n",
      "Episode 9950, Max Tile: 6, Final Score: 888,  Baseline: 1503.39, Improvement: -603.59\n",
      "Episode 10000, Max Tile: 8, Final Score: 2712,  Baseline: 1759.49, Improvement: 975.51\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "model.eval()\n",
    "env = Game2048Env()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_episodes = 10000\n",
    "baseline = 1500\n",
    "batch_size = 25\n",
    "batch_log_probs = []\n",
    "batch_improvs = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    valid_log_probs = []\n",
    "    finalscore = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "        logits = model(state_tensor)\n",
    "\n",
    "        ranked_actions = torch.argsort(logits, dim=1, descending=True)[0]   #sort by how liikely move is\n",
    "\n",
    "        original_board = env.board.copy()\n",
    "        final_action = None\n",
    "        selected_log_prob = None\n",
    "\n",
    "        movecount = 0   #tracks if the first move was valid \n",
    "        for action in ranked_actions:\n",
    "            test_board, _ = move(original_board.copy(), action.item())\n",
    "            if not np.array_equal(test_board, original_board):\n",
    "                final_action = action.item()\n",
    "                dist = Categorical(logits=logits)\n",
    "                selected_log_prob = dist.log_prob(action)\n",
    "                break\n",
    "            movecount += 1\n",
    "\n",
    "        if final_action is None:    #game is stuck, skip (shouldn't happen)\n",
    "            print(\"SOMETHING WRONG AAAAAUUEEUAGHGEUGHHH\")\n",
    "            break\n",
    "\n",
    "        state, score, done = env.step(final_action)\n",
    "        valid_log_probs.append(selected_log_prob)\n",
    "        finalscore += score\n",
    "\n",
    "    # use baseline to force games to improve\n",
    "    baseline = 0.95 * baseline + 0.05 * finalscore\n",
    "    improvement= finalscore - baseline + 0.1 * len(valid_log_probs)\n",
    "\n",
    "    #use batches for more stable training\n",
    "    batch_log_probs.extend(valid_log_probs)\n",
    "    batch_improvs.extend([improvement] * len(valid_log_probs))\n",
    "\n",
    "    if (episode+1) % batch_size == 0:\n",
    "        loss = 0\n",
    "        for log_prob, improvement in zip(batch_log_probs, batch_improvs):\n",
    "            loss -= log_prob * improvement\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_log_probs = []\n",
    "        batch_improvs = []\n",
    "\n",
    "    if (episode+1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1}, Max Tile: {env.board.max()}, Final Score: {finalscore},  Baseline: {baseline:.2f}, Improvement: {improvement:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Finetuned to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_fine.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Finetuned ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "you know the drill...\n",
    "\n",
    "make a new virtual env with the following:\\\n",
    "`python3.9 -m venv tfenv`\n",
    "\n",
    "run this bash script to convert to Tensorflow:\\\n",
    "`pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && onnx-tf convert -i 2048_fine.onnx -o 2048_fine_tf`\n",
    "\n",
    "and then this bash script to convert from Tensorflow to Tensorflow.js:\\\n",
    "`pip install tensorflowjs==4.18.0 && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_fine_tf/ 2048_fine_tfjs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
