{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with RL Finetuning through Self-Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ../data/rot0-mir/rot0_mir_game-1.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-12.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-10.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-3.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-7.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-14.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-5.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-11.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-2.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-13.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-4.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-6.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-15.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_save-1.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-8.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_easy-game.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-9.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_save-2.jsonl\n",
      "file: ../data/rot180/rot180_game-5.jsonl\n",
      "file: ../data/rot180/rot180_game-7.jsonl\n",
      "file: ../data/rot180/rot180_game-3.jsonl\n",
      "file: ../data/rot180/rot180_game-1.jsonl\n",
      "file: ../data/rot180/rot180_game-6.jsonl\n",
      "file: ../data/rot180/rot180_game-4.jsonl\n",
      "file: ../data/rot180/rot180_game-2.jsonl\n",
      "file: ../data/rot180/rot180_game-14.jsonl\n",
      "file: ../data/rot180/rot180_game-12.jsonl\n",
      "file: ../data/rot180/rot180_game-8.jsonl\n",
      "file: ../data/rot180/rot180_save-1.jsonl\n",
      "file: ../data/rot180/rot180_game-10.jsonl\n",
      "file: ../data/rot180/rot180_game-15.jsonl\n",
      "file: ../data/rot180/rot180_game-11.jsonl\n",
      "file: ../data/rot180/rot180_easy-game.jsonl\n",
      "file: ../data/rot180/rot180_game-13.jsonl\n",
      "file: ../data/rot180/rot180_save-2.jsonl\n",
      "file: ../data/rot180/rot180_game-9.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-8.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-13.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-11.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_save-1.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-15.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_easy-game.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-10.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-9.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_save-2.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-12.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-14.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-3.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-1.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-5.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-7.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-2.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-6.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-4.jsonl\n",
      "file: ../data/rot0/rot0_game-15.jsonl\n",
      "file: ../data/rot0/rot0_game-11.jsonl\n",
      "file: ../data/rot0/rot0_save-2.jsonl\n",
      "file: ../data/rot0/rot0_game-9.jsonl\n",
      "file: ../data/rot0/rot0_game-13.jsonl\n",
      "file: ../data/rot0/rot0_game-14.jsonl\n",
      "file: ../data/rot0/rot0_save-1.jsonl\n",
      "file: ../data/rot0/rot0_game-12.jsonl\n",
      "file: ../data/rot0/rot0_game-10.jsonl\n",
      "file: ../data/rot0/rot0_game-8.jsonl\n",
      "file: ../data/rot0/rot0_game-4.jsonl\n",
      "file: ../data/rot0/rot0_game-6.jsonl\n",
      "file: ../data/rot0/rot0_game-2.jsonl\n",
      "file: ../data/rot0/rot0_easy-game.jsonl\n",
      "file: ../data/rot0/rot0_game-7.jsonl\n",
      "file: ../data/rot0/rot0_game-5.jsonl\n",
      "file: ../data/rot0/rot0_game-1.jsonl\n",
      "file: ../data/rot0/rot0_game-3.jsonl\n",
      "file: ../data/raw/game-6.jsonl\n",
      "file: ../data/raw/game-4.jsonl\n",
      "file: ../data/raw/game-2.jsonl\n",
      "file: ../data/raw/game-5.jsonl\n",
      "file: ../data/raw/game-7.jsonl\n",
      "file: ../data/raw/game-3.jsonl\n",
      "file: ../data/raw/game-1.jsonl\n",
      "file: ../data/raw/game-14.jsonl\n",
      "file: ../data/raw/game-12.jsonl\n",
      "file: ../data/raw/save-2.jsonl\n",
      "file: ../data/raw/game-9.jsonl\n",
      "file: ../data/raw/game-10.jsonl\n",
      "file: ../data/raw/easy-game.jsonl\n",
      "file: ../data/raw/game-15.jsonl\n",
      "file: ../data/raw/game-11.jsonl\n",
      "file: ../data/raw/game-8.jsonl\n",
      "file: ../data/raw/save-1.jsonl\n",
      "file: ../data/raw/game-13.jsonl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for subdir in os.listdir(data_dir):\n",
    "    subdir_path = os.path.join(data_dir, subdir)\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            print(f\"file: {file_path}\")\n",
    "            for line in file:\n",
    "                data = json.loads(line.strip())\n",
    "                if \"state\" in data and \"action\" in data:\n",
    "                    X.append(data[\"state\"])\n",
    "                    Y.append(data[\"action\"])\n",
    "X = np.array(X)\n",
    "X[X > 0] = np.log2(X[X > 0])    #replace with log2 for simplicity\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=26)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(Y)   #overwrite with full dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NumPy Arrays to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "X shape: torch.Size([64, 1, 4, 4])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):   #reshape to fit CNN input, -1 to auto infer batch size, 1 for single channel\n",
    "        self.X = torch.from_numpy(X.astype(np.float32)).reshape(-1, 1, 4, 4)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "   \n",
    "batch_size = 64\n",
    "\n",
    "#instantiate training and test data\n",
    "train_data = Data(X_train, y_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test, y_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#sanity check\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "output_dim = 4\n",
    "\n",
    "class CNN(nn.Module):  # Use CNN because input is image-like (4x4 grid)\n",
    "    def __init__(self, output_dim=4, dropout_prob=0.3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 128)\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # add dropout bc i dont have many games\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x) \n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNN(output_dim=output_dim)\n",
    "print(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch 2/30\n",
      "Epoch 3/30\n",
      "Epoch 4/30\n",
      "Epoch 5/30\n",
      "Epoch 6/30\n",
      "Epoch 7/30\n",
      "Epoch 8/30\n",
      "Epoch 9/30\n",
      "Epoch 10/30\n",
      "Epoch 11/30\n",
      "Epoch 12/30\n",
      "Epoch 13/30\n",
      "Epoch 14/30\n",
      "Epoch 15/30\n",
      "Epoch 16/30\n",
      "Epoch 17/30\n",
      "Epoch 18/30\n",
      "Epoch 19/30\n",
      "Epoch 20/30\n",
      "Epoch 21/30\n",
      "Epoch 22/30\n",
      "Epoch 23/30\n",
      "Epoch 24/30\n",
      "Epoch 25/30\n",
      "Epoch 26/30\n",
      "Epoch 27/30\n",
      "Epoch 28/30\n",
      "Epoch 29/30\n",
      "Epoch 30/30\n",
      "DONE!!! :3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "\n",
    "print(\"DONE!!! :3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89%\n",
      "Prediction distribution:\n",
      "0:  94015\n",
      "1:  59257\n",
      "2:  21877\n",
      "3:  62301\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "results = [0,0,0,0]\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get model outputs\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_test.extend(y.tolist())\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        for pred in predicted:\n",
    "            results[pred.item()] += 1\n",
    "\n",
    "print(f'Accuracy: {100 * correct // total}%')\n",
    "print(f'Prediction distribution:')\n",
    "print(f'0:  {results[0]}')\n",
    "print(f'1:  {results[1]}')\n",
    "print(f'2:  {results[2]}')\n",
    "print(f'3:  {results[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Imitation to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_imitation.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Imitation ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "the package versions need to be really specific or else it crashes and burns\n",
    "\n",
    "make two new virtual envs with the following:\\\n",
    "`python3.9 -m venv tfenv && python3.9 -m venv tfjsenv`\n",
    "\n",
    "run this bash script to set up required packages in the two enviornments:\\\n",
    "`source tfenv/bin/activate && pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 &&deactivate && source tfjsenv/bin/activate && pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && pip install tensorflowjs==4.18.0 && deactivate`\n",
    "\n",
    "run this bash script to convert ONNX to Tensorflow, then to Tensorflow.js:\\\n",
    "`source tfenv/bin/activate && cd model && onnx-tf convert -i 2048_imitation.onnx -o 2048_imitation_tf && cd ../ && deactivate && source tfjsenv/bin/activate && cd model && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_imitation_tf/ 2048_imitation_tfjs/ && cd ../ && deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up,right, down, left\n",
    "\n",
    "def add_tile(board):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:   # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_right(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_right(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 2:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 3:  # left\n",
    "        new_board, reward = move_right(board)\n",
    "    elif direction == 1:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Game2048Env:\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(add_tile(self.board))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # old_max_tile = np.max(self.board)\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        changed = not np.array_equal(self.board, old_board)\n",
    "        if changed: # only add a tile if the board changed\n",
    "            self.board = add_tile(self.board)\n",
    "        # new_max_tile = np.max(self.board)\n",
    "        # reward = (new_max_tile > old_max_tile)  # reward for increasing max tile, small reward for merging\n",
    "        done = is_game_over(self.board)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        board = self.board.copy()\n",
    "        board = np.where(board > 0, board, 0)\n",
    "        board = board.astype(np.float32)\n",
    "        board = board.reshape(1, 1, 4, 4)\n",
    "        return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviornment Testing\n",
    "for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Game2048Env()\n",
    "state = env.reset()\n",
    "print(\"Initial board:\")\n",
    "print(env.board)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    print(\"\\nCurrent board:\")\n",
    "    print(env.board)\n",
    "\n",
    "    move_str = input(\"Enter move (w=up, s=down, a=left, d=right, q=quit): \")\n",
    "    if move_str == 'q':\n",
    "        break\n",
    "    move_map = {'w': 0, 'd': 1, 's': 2, 'a': 3}\n",
    "    if move_str not in move_map:\n",
    "        print(\"Invalid input.\")\n",
    "        continue\n",
    "\n",
    "    action = move_map[move_str]\n",
    "    prev_max = np.max(env.board)\n",
    "    state, reward, done = env.step(action)\n",
    "    new_max = np.max(env.board)\n",
    "\n",
    "    print(f\"Action: {move_str.upper()} | Reward: {reward:.2f} | Max tile: {2 ** new_max}\")\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"\\nGame Over.\")\n",
    "print(\"Final board:\")\n",
    "print(env.board)\n",
    "print(f\"Total reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Max Tile: 7, Final Score: 1048,  Baseline: 1149.94, Improvement: -90.74\n",
      "Episode 100, Max Tile: 8, Final Score: 2788,  Baseline: 1019.90, Improvement: 1791.20\n",
      "Episode 150, Max Tile: 6, Final Score: 872,  Baseline: 1272.49, Improvement: -389.39\n",
      "Episode 200, Max Tile: 6, Final Score: 1004,  Baseline: 1298.44, Improvement: -282.64\n",
      "Episode 250, Max Tile: 5, Final Score: 380,  Baseline: 1114.05, Improvement: -727.75\n",
      "Episode 300, Max Tile: 6, Final Score: 740,  Baseline: 998.98, Improvement: -249.18\n",
      "Episode 350, Max Tile: 6, Final Score: 692,  Baseline: 1307.12, Improvement: -606.02\n",
      "Episode 400, Max Tile: 6, Final Score: 1028,  Baseline: 1174.20, Improvement: -134.00\n",
      "Episode 450, Max Tile: 7, Final Score: 1324,  Baseline: 1165.25, Improvement: 172.35\n",
      "Episode 500, Max Tile: 6, Final Score: 1012,  Baseline: 1172.45, Improvement: -148.25\n",
      "Episode 550, Max Tile: 6, Final Score: 500,  Baseline: 1020.89, Improvement: -513.69\n",
      "Episode 600, Max Tile: 8, Final Score: 2496,  Baseline: 1058.60, Improvement: 1458.00\n",
      "Episode 650, Max Tile: 6, Final Score: 504,  Baseline: 1103.45, Improvement: -592.65\n",
      "Episode 700, Max Tile: 6, Final Score: 988,  Baseline: 1070.61, Improvement: -70.91\n",
      "Episode 750, Max Tile: 5, Final Score: 292,  Baseline: 1144.62, Improvement: -847.12\n",
      "Episode 800, Max Tile: 7, Final Score: 1468,  Baseline: 1090.76, Improvement: 392.04\n",
      "Episode 850, Max Tile: 7, Final Score: 912,  Baseline: 1152.53, Improvement: -230.63\n",
      "Episode 900, Max Tile: 6, Final Score: 760,  Baseline: 1040.31, Improvement: -270.11\n",
      "Episode 950, Max Tile: 7, Final Score: 1436,  Baseline: 1056.55, Improvement: 394.25\n",
      "Episode 1000, Max Tile: 6, Final Score: 436,  Baseline: 967.67, Improvement: -525.27\n",
      "Episode 1050, Max Tile: 7, Final Score: 1196,  Baseline: 1096.40, Improvement: 112.10\n",
      "Episode 1100, Max Tile: 6, Final Score: 748,  Baseline: 974.54, Improvement: -216.64\n",
      "Episode 1150, Max Tile: 7, Final Score: 960,  Baseline: 1055.35, Improvement: -84.75\n",
      "Episode 1200, Max Tile: 6, Final Score: 664,  Baseline: 1199.66, Improvement: -526.56\n",
      "Episode 1250, Max Tile: 6, Final Score: 844,  Baseline: 1084.51, Improvement: -229.41\n",
      "Episode 1300, Max Tile: 5, Final Score: 660,  Baseline: 1065.74, Improvement: -396.04\n",
      "Episode 1350, Max Tile: 8, Final Score: 2028,  Baseline: 1120.58, Improvement: 924.32\n",
      "Episode 1400, Max Tile: 7, Final Score: 1420,  Baseline: 1045.02, Improvement: 389.38\n",
      "Episode 1450, Max Tile: 6, Final Score: 772,  Baseline: 1045.42, Improvement: -262.92\n",
      "Episode 1500, Max Tile: 7, Final Score: 1792,  Baseline: 1196.38, Improvement: 614.02\n",
      "Episode 1550, Max Tile: 6, Final Score: 824,  Baseline: 1021.91, Improvement: -187.11\n",
      "Episode 1600, Max Tile: 6, Final Score: 776,  Baseline: 1060.65, Improvement: -274.65\n",
      "Episode 1650, Max Tile: 6, Final Score: 660,  Baseline: 947.83, Improvement: -278.53\n",
      "Episode 1700, Max Tile: 7, Final Score: 1452,  Baseline: 1035.37, Improvement: 431.13\n",
      "Episode 1750, Max Tile: 6, Final Score: 868,  Baseline: 984.33, Improvement: -105.93\n",
      "Episode 1800, Max Tile: 6, Final Score: 732,  Baseline: 1122.16, Improvement: -380.46\n",
      "Episode 1850, Max Tile: 6, Final Score: 520,  Baseline: 1084.47, Improvement: -556.87\n",
      "Episode 1900, Max Tile: 6, Final Score: 648,  Baseline: 1019.24, Improvement: -362.34\n",
      "Episode 1950, Max Tile: 7, Final Score: 1104,  Baseline: 1057.42, Improvement: 58.08\n",
      "Episode 2000, Max Tile: 7, Final Score: 1712,  Baseline: 1004.63, Improvement: 724.27\n",
      "Episode 2050, Max Tile: 8, Final Score: 2132,  Baseline: 1028.40, Improvement: 1121.50\n",
      "Episode 2100, Max Tile: 8, Final Score: 2084,  Baseline: 1069.22, Improvement: 1032.28\n",
      "Episode 2150, Max Tile: 6, Final Score: 804,  Baseline: 1250.79, Improvement: -436.89\n",
      "Episode 2200, Max Tile: 6, Final Score: 620,  Baseline: 1095.00, Improvement: -466.50\n",
      "Episode 2250, Max Tile: 7, Final Score: 1188,  Baseline: 1295.23, Improvement: -95.13\n",
      "Episode 2300, Max Tile: 7, Final Score: 1704,  Baseline: 1160.73, Improvement: 561.07\n",
      "Episode 2350, Max Tile: 7, Final Score: 1464,  Baseline: 1162.60, Improvement: 316.50\n",
      "Episode 2400, Max Tile: 6, Final Score: 680,  Baseline: 1170.26, Improvement: -481.36\n",
      "Episode 2450, Max Tile: 5, Final Score: 592,  Baseline: 970.34, Improvement: -369.34\n",
      "Episode 2500, Max Tile: 7, Final Score: 1488,  Baseline: 1084.03, Improvement: 419.57\n",
      "Episode 2550, Max Tile: 7, Final Score: 1920,  Baseline: 1225.09, Improvement: 713.51\n",
      "Episode 2600, Max Tile: 6, Final Score: 752,  Baseline: 1013.47, Improvement: -251.97\n",
      "Episode 2650, Max Tile: 7, Final Score: 1540,  Baseline: 1049.29, Improvement: 506.61\n",
      "Episode 2700, Max Tile: 7, Final Score: 980,  Baseline: 1029.45, Improvement: -38.75\n",
      "Episode 2750, Max Tile: 6, Final Score: 524,  Baseline: 893.30, Improvement: -361.90\n",
      "Episode 2800, Max Tile: 8, Final Score: 2496,  Baseline: 1083.66, Improvement: 1432.94\n",
      "Episode 2850, Max Tile: 8, Final Score: 2144,  Baseline: 1178.18, Improvement: 983.92\n",
      "Episode 2900, Max Tile: 5, Final Score: 464,  Baseline: 1087.83, Improvement: -616.63\n",
      "Episode 2950, Max Tile: 6, Final Score: 724,  Baseline: 1018.35, Improvement: -284.55\n",
      "Episode 3000, Max Tile: 6, Final Score: 944,  Baseline: 1146.05, Improvement: -191.05\n",
      "Episode 3050, Max Tile: 7, Final Score: 1300,  Baseline: 1147.65, Improvement: 166.35\n",
      "Episode 3100, Max Tile: 6, Final Score: 1004,  Baseline: 1086.76, Improvement: -70.86\n",
      "Episode 3150, Max Tile: 7, Final Score: 1908,  Baseline: 1170.15, Improvement: 757.45\n",
      "Episode 3200, Max Tile: 6, Final Score: 656,  Baseline: 1208.79, Improvement: -544.09\n",
      "Episode 3250, Max Tile: 7, Final Score: 2256,  Baseline: 1077.67, Improvement: 1199.43\n",
      "Episode 3300, Max Tile: 7, Final Score: 1512,  Baseline: 1095.45, Improvement: 432.05\n",
      "Episode 3350, Max Tile: 6, Final Score: 644,  Baseline: 1152.20, Improvement: -499.80\n",
      "Episode 3400, Max Tile: 6, Final Score: 1300,  Baseline: 1038.24, Improvement: 276.36\n",
      "Episode 3450, Max Tile: 8, Final Score: 2008,  Baseline: 1187.64, Improvement: 837.16\n",
      "Episode 3500, Max Tile: 7, Final Score: 1260,  Baseline: 1169.81, Improvement: 103.49\n",
      "Episode 3550, Max Tile: 8, Final Score: 2744,  Baseline: 1202.45, Improvement: 1564.95\n",
      "Episode 3600, Max Tile: 6, Final Score: 648,  Baseline: 944.34, Improvement: -287.64\n",
      "Episode 3650, Max Tile: 6, Final Score: 900,  Baseline: 1183.48, Improvement: -272.48\n",
      "Episode 3700, Max Tile: 7, Final Score: 1392,  Baseline: 1043.04, Improvement: 363.16\n",
      "Episode 3750, Max Tile: 7, Final Score: 916,  Baseline: 1272.59, Improvement: -346.59\n",
      "Episode 3800, Max Tile: 7, Final Score: 1728,  Baseline: 1343.03, Improvement: 402.37\n",
      "Episode 3850, Max Tile: 7, Final Score: 1272,  Baseline: 1131.46, Improvement: 153.84\n",
      "Episode 3900, Max Tile: 7, Final Score: 1404,  Baseline: 1258.89, Improvement: 159.71\n",
      "Episode 3950, Max Tile: 6, Final Score: 732,  Baseline: 1282.95, Improvement: -541.35\n",
      "Episode 4000, Max Tile: 6, Final Score: 708,  Baseline: 916.91, Improvement: -199.41\n",
      "Episode 4050, Max Tile: 7, Final Score: 2340,  Baseline: 1183.53, Improvement: 1178.77\n",
      "Episode 4100, Max Tile: 8, Final Score: 2320,  Baseline: 1364.99, Improvement: 974.91\n",
      "Episode 4150, Max Tile: 7, Final Score: 1504,  Baseline: 1206.07, Improvement: 312.83\n",
      "Episode 4200, Max Tile: 7, Final Score: 1740,  Baseline: 1206.50, Improvement: 551.10\n",
      "Episode 4250, Max Tile: 6, Final Score: 1132,  Baseline: 1234.14, Improvement: -88.54\n",
      "Episode 4300, Max Tile: 6, Final Score: 844,  Baseline: 1064.12, Improvement: -208.92\n",
      "Episode 4350, Max Tile: 7, Final Score: 1016,  Baseline: 934.63, Improvement: 92.77\n",
      "Episode 4400, Max Tile: 7, Final Score: 1328,  Baseline: 1057.94, Improvement: 284.86\n",
      "Episode 4450, Max Tile: 5, Final Score: 620,  Baseline: 1107.80, Improvement: -478.40\n",
      "Episode 4500, Max Tile: 7, Final Score: 1356,  Baseline: 1133.01, Improvement: 236.99\n",
      "Episode 4550, Max Tile: 8, Final Score: 2432,  Baseline: 1434.78, Improvement: 1018.42\n",
      "Episode 4600, Max Tile: 6, Final Score: 1252,  Baseline: 1141.30, Improvement: 125.20\n",
      "Episode 4650, Max Tile: 6, Final Score: 748,  Baseline: 1062.68, Improvement: -304.68\n",
      "Episode 4700, Max Tile: 8, Final Score: 2760,  Baseline: 1541.04, Improvement: 1243.06\n",
      "Episode 4750, Max Tile: 8, Final Score: 3480,  Baseline: 1348.73, Improvement: 2160.27\n",
      "Episode 4800, Max Tile: 7, Final Score: 1480,  Baseline: 1184.70, Improvement: 310.90\n",
      "Episode 4850, Max Tile: 6, Final Score: 828,  Baseline: 1221.58, Improvement: -382.78\n",
      "Episode 4900, Max Tile: 6, Final Score: 732,  Baseline: 1241.39, Improvement: -499.39\n",
      "Episode 4950, Max Tile: 6, Final Score: 752,  Baseline: 1033.41, Improvement: -272.01\n",
      "Episode 5000, Max Tile: 5, Final Score: 276,  Baseline: 916.01, Improvement: -634.71\n",
      "Episode 5050, Max Tile: 8, Final Score: 3196,  Baseline: 1190.66, Improvement: 2031.94\n",
      "Episode 5100, Max Tile: 5, Final Score: 476,  Baseline: 1312.01, Improvement: -828.61\n",
      "Episode 5150, Max Tile: 7, Final Score: 1196,  Baseline: 1147.98, Improvement: 60.42\n",
      "Episode 5200, Max Tile: 7, Final Score: 1424,  Baseline: 1298.36, Improvement: 140.24\n",
      "Episode 5250, Max Tile: 8, Final Score: 2084,  Baseline: 1469.63, Improvement: 632.27\n",
      "Episode 5300, Max Tile: 7, Final Score: 1324,  Baseline: 1423.28, Improvement: -85.58\n",
      "Episode 5350, Max Tile: 7, Final Score: 1212,  Baseline: 1182.13, Improvement: 43.27\n",
      "Episode 5400, Max Tile: 5, Final Score: 452,  Baseline: 1157.09, Improvement: -698.29\n",
      "Episode 5450, Max Tile: 8, Final Score: 2176,  Baseline: 1240.92, Improvement: 953.58\n",
      "Episode 5500, Max Tile: 7, Final Score: 1604,  Baseline: 1463.00, Improvement: 156.90\n",
      "Episode 5550, Max Tile: 6, Final Score: 716,  Baseline: 1405.17, Improvement: -679.67\n",
      "Episode 5600, Max Tile: 7, Final Score: 1292,  Baseline: 1241.86, Improvement: 63.44\n",
      "Episode 5650, Max Tile: 7, Final Score: 1504,  Baseline: 1459.56, Improvement: 60.04\n",
      "Episode 5700, Max Tile: 8, Final Score: 3172,  Baseline: 1455.88, Improvement: 1742.72\n",
      "Episode 5750, Max Tile: 7, Final Score: 1220,  Baseline: 1236.55, Improvement: -4.05\n",
      "Episode 5800, Max Tile: 5, Final Score: 416,  Baseline: 1053.01, Improvement: -630.11\n",
      "Episode 5850, Max Tile: 7, Final Score: 1440,  Baseline: 1349.71, Improvement: 105.19\n",
      "Episode 5900, Max Tile: 6, Final Score: 1028,  Baseline: 1233.60, Improvement: -193.50\n",
      "Episode 5950, Max Tile: 7, Final Score: 1144,  Baseline: 1405.74, Improvement: -249.44\n",
      "Episode 6000, Max Tile: 8, Final Score: 2880,  Baseline: 1612.23, Improvement: 1291.07\n",
      "Episode 6050, Max Tile: 7, Final Score: 1452,  Baseline: 1193.04, Improvement: 273.56\n",
      "Episode 6100, Max Tile: 8, Final Score: 3512,  Baseline: 1441.04, Improvement: 2101.16\n",
      "Episode 6150, Max Tile: 7, Final Score: 1164,  Baseline: 1444.17, Improvement: -267.67\n",
      "Episode 6200, Max Tile: 5, Final Score: 380,  Baseline: 1668.11, Improvement: -1281.91\n",
      "Episode 6250, Max Tile: 5, Final Score: 412,  Baseline: 1435.43, Improvement: -1016.53\n",
      "Episode 6300, Max Tile: 9, Final Score: 4840,  Baseline: 1388.25, Improvement: 3485.95\n",
      "Episode 6350, Max Tile: 8, Final Score: 2996,  Baseline: 1840.63, Improvement: 1180.87\n",
      "Episode 6400, Max Tile: 6, Final Score: 1420,  Baseline: 1453.15, Improvement: -17.35\n",
      "Episode 6450, Max Tile: 8, Final Score: 2180,  Baseline: 1665.54, Improvement: 533.06\n",
      "Episode 6500, Max Tile: 7, Final Score: 1800,  Baseline: 1341.46, Improvement: 477.04\n",
      "Episode 6550, Max Tile: 5, Final Score: 420,  Baseline: 1547.05, Improvement: -1119.75\n",
      "Episode 6600, Max Tile: 7, Final Score: 1764,  Baseline: 1396.02, Improvement: 386.78\n",
      "Episode 6650, Max Tile: 7, Final Score: 1488,  Baseline: 1639.04, Improvement: -135.34\n",
      "Episode 6700, Max Tile: 7, Final Score: 1240,  Baseline: 1781.93, Improvement: -529.13\n",
      "Episode 6750, Max Tile: 7, Final Score: 1336,  Baseline: 1379.93, Improvement: -30.03\n",
      "Episode 6800, Max Tile: 7, Final Score: 1132,  Baseline: 1392.59, Improvement: -248.59\n",
      "Episode 6850, Max Tile: 7, Final Score: 1880,  Baseline: 1640.49, Improvement: 257.51\n",
      "Episode 6900, Max Tile: 8, Final Score: 3836,  Baseline: 1459.78, Improvement: 2407.72\n",
      "Episode 6950, Max Tile: 7, Final Score: 1524,  Baseline: 1399.17, Improvement: 140.63\n",
      "Episode 7000, Max Tile: 7, Final Score: 1564,  Baseline: 1594.95, Improvement: -14.65\n",
      "Episode 7050, Max Tile: 6, Final Score: 972,  Baseline: 1601.77, Improvement: -617.97\n",
      "Episode 7100, Max Tile: 6, Final Score: 768,  Baseline: 1569.67, Improvement: -791.87\n",
      "Episode 7150, Max Tile: 7, Final Score: 1708,  Baseline: 1574.96, Improvement: 150.64\n",
      "Episode 7200, Max Tile: 6, Final Score: 824,  Baseline: 1390.05, Improvement: -556.25\n",
      "Episode 7250, Max Tile: 6, Final Score: 588,  Baseline: 1603.05, Improvement: -1006.95\n",
      "Episode 7300, Max Tile: 7, Final Score: 1428,  Baseline: 1529.54, Improvement: -86.84\n",
      "Episode 7350, Max Tile: 6, Final Score: 700,  Baseline: 1615.42, Improvement: -906.42\n",
      "Episode 7400, Max Tile: 5, Final Score: 504,  Baseline: 1322.06, Improvement: -810.46\n",
      "Episode 7450, Max Tile: 8, Final Score: 2088,  Baseline: 1335.23, Improvement: 770.27\n",
      "Episode 7500, Max Tile: 7, Final Score: 1404,  Baseline: 1389.56, Improvement: 28.94\n",
      "Episode 7550, Max Tile: 7, Final Score: 1200,  Baseline: 1542.35, Improvement: -329.35\n",
      "Episode 7600, Max Tile: 8, Final Score: 2984,  Baseline: 1640.24, Improvement: 1368.66\n",
      "Episode 7650, Max Tile: 7, Final Score: 2632,  Baseline: 1577.26, Improvement: 1077.64\n",
      "Episode 7700, Max Tile: 6, Final Score: 708,  Baseline: 1510.52, Improvement: -793.12\n",
      "Episode 7750, Max Tile: 7, Final Score: 1448,  Baseline: 1727.38, Improvement: -264.38\n",
      "Episode 7800, Max Tile: 7, Final Score: 1708,  Baseline: 1611.69, Improvement: 113.41\n",
      "Episode 7850, Max Tile: 6, Final Score: 872,  Baseline: 1664.51, Improvement: -781.21\n",
      "Episode 7900, Max Tile: 6, Final Score: 580,  Baseline: 1513.40, Improvement: -925.50\n",
      "Episode 7950, Max Tile: 7, Final Score: 1656,  Baseline: 1315.79, Improvement: 356.71\n",
      "Episode 8000, Max Tile: 7, Final Score: 1432,  Baseline: 1747.87, Improvement: -301.17\n",
      "Episode 8050, Max Tile: 6, Final Score: 676,  Baseline: 1407.83, Improvement: -722.83\n",
      "Episode 8100, Max Tile: 8, Final Score: 2644,  Baseline: 1833.57, Improvement: 833.63\n",
      "Episode 8150, Max Tile: 7, Final Score: 1148,  Baseline: 1927.83, Improvement: -767.73\n",
      "Episode 8200, Max Tile: 7, Final Score: 1656,  Baseline: 1512.97, Improvement: 160.03\n",
      "Episode 8250, Max Tile: 7, Final Score: 1292,  Baseline: 1632.82, Improvement: -327.82\n",
      "Episode 8300, Max Tile: 8, Final Score: 2412,  Baseline: 1418.85, Improvement: 1013.15\n",
      "Episode 8350, Max Tile: 8, Final Score: 3448,  Baseline: 1763.73, Improvement: 1712.47\n",
      "Episode 8400, Max Tile: 6, Final Score: 788,  Baseline: 1561.65, Improvement: -764.05\n",
      "Episode 8450, Max Tile: 6, Final Score: 828,  Baseline: 1611.86, Improvement: -773.16\n",
      "Episode 8500, Max Tile: 7, Final Score: 2100,  Baseline: 1737.50, Improvement: 382.20\n",
      "Episode 8550, Max Tile: 8, Final Score: 2916,  Baseline: 1551.73, Improvement: 1389.77\n",
      "Episode 8600, Max Tile: 8, Final Score: 2168,  Baseline: 1700.42, Improvement: 485.68\n",
      "Episode 8650, Max Tile: 7, Final Score: 1400,  Baseline: 1580.93, Improvement: -166.53\n",
      "Episode 8700, Max Tile: 7, Final Score: 1304,  Baseline: 1843.12, Improvement: -525.62\n",
      "Episode 8750, Max Tile: 6, Final Score: 764,  Baseline: 1656.58, Improvement: -882.68\n",
      "Episode 8800, Max Tile: 8, Final Score: 2712,  Baseline: 2057.68, Improvement: 676.12\n",
      "Episode 8850, Max Tile: 8, Final Score: 2644,  Baseline: 1927.14, Improvement: 739.36\n",
      "Episode 8900, Max Tile: 7, Final Score: 1952,  Baseline: 1578.18, Improvement: 394.02\n",
      "Episode 8950, Max Tile: 5, Final Score: 808,  Baseline: 1705.26, Improvement: -885.86\n",
      "Episode 9000, Max Tile: 6, Final Score: 416,  Baseline: 1658.18, Improvement: -1236.08\n",
      "Episode 9050, Max Tile: 8, Final Score: 2752,  Baseline: 1930.73, Improvement: 844.87\n",
      "Episode 9100, Max Tile: 6, Final Score: 748,  Baseline: 1765.16, Improvement: -1007.26\n",
      "Episode 9150, Max Tile: 6, Final Score: 832,  Baseline: 1888.46, Improvement: -1045.66\n",
      "Episode 9200, Max Tile: 6, Final Score: 1096,  Baseline: 1551.11, Improvement: -442.51\n",
      "Episode 9250, Max Tile: 7, Final Score: 1480,  Baseline: 1592.31, Improvement: -96.91\n",
      "Episode 9300, Max Tile: 7, Final Score: 1376,  Baseline: 1568.45, Improvement: -177.95\n",
      "Episode 9350, Max Tile: 6, Final Score: 612,  Baseline: 1889.65, Improvement: -1269.35\n",
      "Episode 9400, Max Tile: 6, Final Score: 744,  Baseline: 1580.29, Improvement: -826.99\n",
      "Episode 9450, Max Tile: 8, Final Score: 2460,  Baseline: 1597.95, Improvement: 883.15\n",
      "Episode 9500, Max Tile: 9, Final Score: 5540,  Baseline: 2193.82, Improvement: 3385.18\n",
      "Episode 9550, Max Tile: 5, Final Score: 404,  Baseline: 1905.62, Improvement: -1494.92\n",
      "Episode 9600, Max Tile: 7, Final Score: 1604,  Baseline: 1874.46, Improvement: -254.26\n",
      "Episode 9650, Max Tile: 6, Final Score: 1004,  Baseline: 1897.16, Improvement: -881.06\n",
      "Episode 9700, Max Tile: 8, Final Score: 3912,  Baseline: 1650.96, Improvement: 2292.84\n",
      "Episode 9750, Max Tile: 7, Final Score: 1476,  Baseline: 1839.28, Improvement: -347.58\n",
      "Episode 9800, Max Tile: 6, Final Score: 960,  Baseline: 1596.03, Improvement: -623.73\n",
      "Episode 9850, Max Tile: 8, Final Score: 2320,  Baseline: 1996.92, Improvement: 343.48\n",
      "Episode 9900, Max Tile: 8, Final Score: 2712,  Baseline: 1946.46, Improvement: 789.64\n",
      "Episode 9950, Max Tile: 7, Final Score: 1944,  Baseline: 1997.33, Improvement: -33.63\n",
      "Episode 10000, Max Tile: 5, Final Score: 280,  Baseline: 1694.08, Improvement: -1408.78\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "model.eval()\n",
    "env = Game2048Env()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_episodes = 10000\n",
    "baseline = 1500\n",
    "batch_size = 25\n",
    "batch_log_probs = []\n",
    "batch_improvs = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    valid_log_probs = []\n",
    "    finalscore = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "        logits = model(state_tensor)\n",
    "\n",
    "        ranked_actions = torch.argsort(logits, dim=1, descending=True)[0]   #sort by how liikely move is\n",
    "\n",
    "        original_board = env.board.copy()\n",
    "        final_action = None\n",
    "        selected_log_prob = None\n",
    "\n",
    "        movecount = 0   #tracks if the first move was valid \n",
    "        for action in ranked_actions:\n",
    "            test_board, _ = move(original_board.copy(), action.item())\n",
    "            if not np.array_equal(test_board, original_board):\n",
    "                final_action = action.item()\n",
    "                dist = Categorical(logits=logits)\n",
    "                selected_log_prob = dist.log_prob(action)\n",
    "                break\n",
    "            movecount += 1\n",
    "\n",
    "        if final_action is None:    #game is stuck, skip (shouldn't happen)\n",
    "            print(\"SOMETHING WRONG AAAAAUUEEUAGHGEUGHHH\")\n",
    "            break\n",
    "\n",
    "        state, score, done = env.step(final_action)\n",
    "        valid_log_probs.append(selected_log_prob)\n",
    "        finalscore += score\n",
    "\n",
    "    # use baseline to force games to improve\n",
    "    baseline = 0.95 * baseline + 0.05 * finalscore\n",
    "    improvement= finalscore - baseline + 0.1 * len(valid_log_probs)\n",
    "\n",
    "    #use batches for more stable training\n",
    "    batch_log_probs.extend(valid_log_probs)\n",
    "    batch_improvs.extend([improvement] * len(valid_log_probs))\n",
    "\n",
    "    if (episode+1) % batch_size == 0:\n",
    "        loss = 0\n",
    "        for log_prob, improvement in zip(batch_log_probs, batch_improvs):\n",
    "            loss -= log_prob * improvement\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_log_probs = []\n",
    "        batch_improvs = []\n",
    "\n",
    "    if (episode+1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1}, Max Tile: {env.board.max()}, Final Score: {finalscore},  Baseline: {baseline:.2f}, Improvement: {improvement:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Finetuned to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_fine.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Finetuned ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "you know the drill...\n",
    "\n",
    "run this bash script to convert ONNX to Tensorflow to Tensorflow.js:\\\n",
    "`source tfenv/bin/activate && cd model && onnx-tf convert -i 2048_fine.onnx -o 2048_fine_tf && cd ../ && deactivate && source tfjs/bin/activate && cd model && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_fine_tf/ 2048_fine_tfjs/ && cd ../ && deactivate`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
