{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with RL Finetuning through Self-Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ../data/rot90/rot90_game-4.jsonl\n",
      "file: ../data/rot90/rot90_game-6.jsonl\n",
      "file: ../data/rot90/rot90_game-2.jsonl\n",
      "file: ../data/rot90/rot90_game-7.jsonl\n",
      "file: ../data/rot90/rot90_game-5.jsonl\n",
      "file: ../data/rot90/rot90_game-1.jsonl\n",
      "file: ../data/rot90/rot90_game-3.jsonl\n",
      "file: ../data/rot90/rot90_game-9.jsonl\n",
      "file: ../data/rot90/rot90_game-10.jsonl\n",
      "file: ../data/rot90/rot90_game-8.jsonl\n",
      "file: ../data/rot90/rot90_easy-game.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-1.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-10.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-3.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-7.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-5.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-2.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-4.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-6.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-8.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_easy-game.jsonl\n",
      "file: ../data/rot0-mir/rot0_mir_game-9.jsonl\n",
      "file: ../data/rot180/rot180_game-5.jsonl\n",
      "file: ../data/rot180/rot180_game-7.jsonl\n",
      "file: ../data/rot180/rot180_game-3.jsonl\n",
      "file: ../data/rot180/rot180_game-1.jsonl\n",
      "file: ../data/rot180/rot180_game-6.jsonl\n",
      "file: ../data/rot180/rot180_game-4.jsonl\n",
      "file: ../data/rot180/rot180_game-2.jsonl\n",
      "file: ../data/rot180/rot180_game-8.jsonl\n",
      "file: ../data/rot180/rot180_game-10.jsonl\n",
      "file: ../data/rot180/rot180_easy-game.jsonl\n",
      "file: ../data/rot180/rot180_game-9.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-8.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_easy-game.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-10.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-9.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-3.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-1.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-5.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-7.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-2.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-6.jsonl\n",
      "file: ../data/rot180-mir/rot180_mir_game-4.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-8.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-9.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_easy-game.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-10.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-3.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-1.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-5.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-7.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-2.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-6.jsonl\n",
      "file: ../data/rot90-mir/rot90_mir_game-4.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-7.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-5.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-1.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-3.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-4.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-6.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-2.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-10.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_easy-game.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-8.jsonl\n",
      "file: ../data/rot270-mir/rot270_mir_game-9.jsonl\n",
      "file: ../data/raw/game-6.jsonl\n",
      "file: ../data/raw/game-4.jsonl\n",
      "file: ../data/raw/game-2.jsonl\n",
      "file: ../data/raw/game-5.jsonl\n",
      "file: ../data/raw/game-7.jsonl\n",
      "file: ../data/raw/game-3.jsonl\n",
      "file: ../data/raw/game-1.jsonl\n",
      "file: ../data/raw/game-9.jsonl\n",
      "file: ../data/raw/game-10.jsonl\n",
      "file: ../data/raw/easy-game.jsonl\n",
      "file: ../data/raw/game-8.jsonl\n",
      "file: ../data/rot270/rot270_game-10.jsonl\n",
      "file: ../data/rot270/rot270_game-9.jsonl\n",
      "file: ../data/rot270/rot270_game-8.jsonl\n",
      "file: ../data/rot270/rot270_game-4.jsonl\n",
      "file: ../data/rot270/rot270_game-6.jsonl\n",
      "file: ../data/rot270/rot270_game-2.jsonl\n",
      "file: ../data/rot270/rot270_easy-game.jsonl\n",
      "file: ../data/rot270/rot270_game-7.jsonl\n",
      "file: ../data/rot270/rot270_game-5.jsonl\n",
      "file: ../data/rot270/rot270_game-1.jsonl\n",
      "file: ../data/rot270/rot270_game-3.jsonl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for subdir in os.listdir(data_dir):\n",
    "    subdir_path = os.path.join(data_dir, subdir)\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            print(f\"file: {file_path}\")\n",
    "            for line in file:\n",
    "                data = json.loads(line.strip())\n",
    "                if \"state\" in data and \"action\" in data:\n",
    "                    X.append(data[\"state\"])\n",
    "                    Y.append(data[\"action\"])\n",
    "X = np.array(X)\n",
    "X[X > 0] = np.log2(X[X > 0])    #replace with log2 for simplicity\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.33, random_state=26)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(Y)   #overwrite with full dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NumPy Arrays to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "X shape: torch.Size([64, 1, 4, 4])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):   #reshape to fit CNN input, -1 to auto infer batch size, 1 for single channel\n",
    "        self.X = torch.from_numpy(X.astype(np.float32)).reshape(-1, 1, 4, 4)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "   \n",
    "batch_size = 64\n",
    "\n",
    "#instantiate training and test data\n",
    "train_data = Data(X_train, y_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test, y_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#sanity check\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = 16\n",
    "hidden_dim1 = 256\n",
    "hidden_dim2 = 128\n",
    "output_dim = 4\n",
    "\n",
    "class CNN(nn.Module):   #use CNN because input is image-like (4x4 grid)\n",
    "    def __init__(self, output_dim=4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(32 * 2 * 2, 64)  # final output size after convs\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = CNN(output_dim=output_dim)\n",
    "print(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Epoch 2/30\n",
      "Epoch 3/30\n",
      "Epoch 4/30\n",
      "Epoch 5/30\n",
      "Epoch 6/30\n",
      "Epoch 7/30\n",
      "Epoch 8/30\n",
      "Epoch 9/30\n",
      "Epoch 10/30\n",
      "Epoch 11/30\n",
      "Epoch 12/30\n",
      "Epoch 13/30\n",
      "Epoch 14/30\n",
      "Epoch 15/30\n",
      "Epoch 16/30\n",
      "Epoch 17/30\n",
      "Epoch 18/30\n",
      "Epoch 19/30\n",
      "Epoch 20/30\n",
      "Epoch 21/30\n",
      "Epoch 22/30\n",
      "Epoch 23/30\n",
      "Epoch 24/30\n",
      "Epoch 25/30\n",
      "Epoch 26/30\n",
      "Epoch 27/30\n",
      "Epoch 28/30\n",
      "Epoch 29/30\n",
      "Epoch 30/30\n",
      "DONE!!! :3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "\n",
    "print(\"DONE!!! :3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83%\n",
      "Prediction distribution:\n",
      "0:  74746\n",
      "1:  79137\n",
      "2:  72626\n",
      "3:  73984\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "results = [0,0,0,0]\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get model outputs\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_test.extend(y.tolist())\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        for pred in predicted:\n",
    "            results[pred.item()] += 1\n",
    "\n",
    "print(f'Accuracy: {100 * correct // total}%')\n",
    "print(f'Prediction distribution:')\n",
    "print(f'0:  {results[0]}')\n",
    "print(f'1:  {results[1]}')\n",
    "print(f'2:  {results[2]}')\n",
    "print(f'3:  {results[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Imitation to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_imitation.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Imitation ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "the package versions need to be really specific or else it crashes and burns\n",
    "\n",
    "make a new virtual env with the following:\\\n",
    "`python3.9 -m venv tfenv`\n",
    "\n",
    "run this bash script to convert to Tensorflow:\\\n",
    "`pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && onnx-tf convert -i 2048_imitation.onnx -o 2048_imitation_tf`\n",
    "\n",
    "and then this bash script to convert from Tensorflow to Tensorflow.js:\\\n",
    "`pip install tensorflowjs==4.18.0 && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_imitation_tf/ 2048_imitation_tfjs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "ACTIONS = [0, 1, 2, 3]  # up, down, left, right\n",
    "\n",
    "def add_tile(board):\n",
    "    empty = list(zip(*np.where(board == 0)))\n",
    "    if not empty:   # no empty cells\n",
    "        return board\n",
    "    y, x = random.choice(empty)\n",
    "    board[y][x] = 1 if random.random() < 0.9 else 2\n",
    "    return board\n",
    "\n",
    "def move_right(board):\n",
    "    new_board = np.zeros_like(board)\n",
    "    reward = 0\n",
    "    for row in range(BOARD_SIZE):\n",
    "        tiles = board[row][board[row] != 0] # collect non-zero tiles\n",
    "        merged = []\n",
    "        skip = False\n",
    "        for i in range(len(tiles)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i + 1 < len(tiles) and tiles[i] == tiles[i+1]:\n",
    "                merged.append(tiles[i] + 1)\n",
    "                reward += 2 ** (tiles[i] + 1)  # calculate reward\n",
    "                skip = True\n",
    "            else:\n",
    "                merged.append(tiles[i])\n",
    "        new_board[row][:len(merged)] = merged\n",
    "    return new_board, reward\n",
    "\n",
    "def move(board, direction): \n",
    "    if direction == 0:  # up\n",
    "        board = np.rot90(board, 1)\n",
    "        new_board, reward = move_right(board)   #reuse this func to death bc im lazy lmao\n",
    "        new_board = np.rot90(new_board, -1)\n",
    "    elif direction == 1:  # down\n",
    "        board = np.rot90(board, -1)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.rot90(new_board)\n",
    "    elif direction == 2:  # left\n",
    "        new_board, reward = move_right(board)\n",
    "    elif direction == 3:  # right\n",
    "        board = np.fliplr(board)\n",
    "        new_board, reward = move_right(board)\n",
    "        new_board = np.fliplr(new_board)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "    return new_board, reward\n",
    "\n",
    "def is_game_over(board):\n",
    "    for a in ACTIONS:\n",
    "        new_board, _ = move(board, a)\n",
    "        if not np.array_equal(new_board, board):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Game2048Env:\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
    "        self.board = add_tile(add_tile(self.board))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # old_max_tile = np.max(self.board)\n",
    "        old_board = self.board.copy()\n",
    "        self.board, reward = move(self.board, action)\n",
    "        changed = not np.array_equal(self.board, old_board)\n",
    "        if changed: # only add a tile if the board changed\n",
    "            self.board = add_tile(self.board)\n",
    "        # new_max_tile = np.max(self.board)\n",
    "        # reward = (new_max_tile > old_max_tile)  # reward for increasing max tile, small reward for merging\n",
    "        done = is_game_over(self.board)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def get_state(self):\n",
    "        board = self.board.copy()\n",
    "        board = np.where(board > 0, board, 0)\n",
    "        board = board.astype(np.float32)\n",
    "        board = board.reshape(1, 1, 4, 4)\n",
    "        return board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Max Tile: 8, Final Score: 2088,  Baseline: 986.10, Improvement: 1119.80\n",
      "Episode 100, Max Tile: 6, Final Score: 920,  Baseline: 918.13, Improvement: 12.67\n",
      "Episode 150, Max Tile: 7, Final Score: 1064,  Baseline: 828.26, Improvement: 247.14\n",
      "Episode 200, Max Tile: 7, Final Score: 1152,  Baseline: 930.91, Improvement: 233.49\n",
      "Episode 250, Max Tile: 8, Final Score: 2644,  Baseline: 964.70, Improvement: 1700.50\n",
      "Episode 300, Max Tile: 7, Final Score: 960,  Baseline: 942.65, Improvement: 27.45\n",
      "Episode 350, Max Tile: 6, Final Score: 752,  Baseline: 893.35, Improvement: -131.25\n",
      "Episode 400, Max Tile: 7, Final Score: 1272,  Baseline: 928.27, Improvement: 356.33\n",
      "Episode 450, Max Tile: 7, Final Score: 1076,  Baseline: 931.97, Improvement: 155.83\n",
      "Episode 500, Max Tile: 7, Final Score: 1412,  Baseline: 970.09, Improvement: 456.61\n",
      "Episode 550, Max Tile: 7, Final Score: 1416,  Baseline: 795.54, Improvement: 635.16\n",
      "Episode 600, Max Tile: 7, Final Score: 1228,  Baseline: 1001.23, Improvement: 239.97\n",
      "Episode 650, Max Tile: 7, Final Score: 1324,  Baseline: 810.14, Improvement: 527.56\n",
      "Episode 700, Max Tile: 5, Final Score: 500,  Baseline: 772.25, Improvement: -264.75\n",
      "Episode 750, Max Tile: 6, Final Score: 960,  Baseline: 868.44, Improvement: 102.96\n",
      "Episode 800, Max Tile: 7, Final Score: 1400,  Baseline: 942.23, Improvement: 472.17\n",
      "Episode 850, Max Tile: 7, Final Score: 1536,  Baseline: 910.76, Improvement: 640.64\n",
      "Episode 900, Max Tile: 6, Final Score: 732,  Baseline: 914.87, Improvement: -173.07\n",
      "Episode 950, Max Tile: 6, Final Score: 580,  Baseline: 930.86, Improvement: -342.96\n",
      "Episode 1000, Max Tile: 6, Final Score: 840,  Baseline: 840.79, Improvement: 10.31\n",
      "Episode 1050, Max Tile: 6, Final Score: 652,  Baseline: 820.84, Improvement: -160.34\n",
      "Episode 1100, Max Tile: 6, Final Score: 596,  Baseline: 871.41, Improvement: -267.21\n",
      "Episode 1150, Max Tile: 5, Final Score: 584,  Baseline: 822.75, Improvement: -229.75\n",
      "Episode 1200, Max Tile: 6, Final Score: 688,  Baseline: 866.79, Improvement: -169.69\n",
      "Episode 1250, Max Tile: 7, Final Score: 1000,  Baseline: 968.73, Improvement: 42.37\n",
      "Episode 1300, Max Tile: 6, Final Score: 984,  Baseline: 875.59, Improvement: 119.91\n",
      "Episode 1350, Max Tile: 7, Final Score: 1180,  Baseline: 912.22, Improvement: 280.18\n",
      "Episode 1400, Max Tile: 5, Final Score: 528,  Baseline: 1012.27, Improvement: -476.17\n",
      "Episode 1450, Max Tile: 7, Final Score: 1324,  Baseline: 903.99, Improvement: 433.81\n",
      "Episode 1500, Max Tile: 6, Final Score: 864,  Baseline: 907.07, Improvement: -32.77\n",
      "Episode 1550, Max Tile: 6, Final Score: 1008,  Baseline: 865.58, Improvement: 154.52\n",
      "Episode 1600, Max Tile: 5, Final Score: 336,  Baseline: 899.07, Improvement: -557.27\n",
      "Episode 1650, Max Tile: 7, Final Score: 1364,  Baseline: 896.48, Improvement: 482.02\n",
      "Episode 1700, Max Tile: 6, Final Score: 872,  Baseline: 896.87, Improvement: -14.27\n",
      "Episode 1750, Max Tile: 6, Final Score: 900,  Baseline: 871.89, Improvement: 39.11\n",
      "Episode 1800, Max Tile: 6, Final Score: 648,  Baseline: 818.62, Improvement: -161.52\n",
      "Episode 1850, Max Tile: 8, Final Score: 2260,  Baseline: 986.85, Improvement: 1292.25\n",
      "Episode 1900, Max Tile: 7, Final Score: 1220,  Baseline: 888.97, Improvement: 343.43\n",
      "Episode 1950, Max Tile: 5, Final Score: 252,  Baseline: 780.30, Improvement: -523.30\n",
      "Episode 2000, Max Tile: 6, Final Score: 744,  Baseline: 928.85, Improvement: -175.45\n",
      "Episode 2050, Max Tile: 6, Final Score: 924,  Baseline: 872.48, Improvement: 62.72\n",
      "Episode 2100, Max Tile: 6, Final Score: 620,  Baseline: 768.06, Improvement: -139.76\n",
      "Episode 2150, Max Tile: 6, Final Score: 640,  Baseline: 850.66, Improvement: -201.86\n",
      "Episode 2200, Max Tile: 6, Final Score: 620,  Baseline: 892.11, Improvement: -263.51\n",
      "Episode 2250, Max Tile: 6, Final Score: 572,  Baseline: 925.40, Improvement: -345.50\n",
      "Episode 2300, Max Tile: 7, Final Score: 1080,  Baseline: 967.13, Improvement: 124.47\n",
      "Episode 2350, Max Tile: 7, Final Score: 1036,  Baseline: 904.23, Improvement: 143.07\n",
      "Episode 2400, Max Tile: 5, Final Score: 280,  Baseline: 849.66, Improvement: -564.26\n",
      "Episode 2450, Max Tile: 7, Final Score: 1652,  Baseline: 915.90, Improvement: 752.80\n",
      "Episode 2500, Max Tile: 6, Final Score: 660,  Baseline: 815.21, Improvement: -146.41\n",
      "Episode 2550, Max Tile: 5, Final Score: 428,  Baseline: 952.07, Improvement: -517.47\n",
      "Episode 2600, Max Tile: 8, Final Score: 2256,  Baseline: 874.41, Improvement: 1399.89\n",
      "Episode 2650, Max Tile: 7, Final Score: 1552,  Baseline: 1006.26, Improvement: 561.84\n",
      "Episode 2700, Max Tile: 6, Final Score: 840,  Baseline: 1003.62, Improvement: -152.92\n",
      "Episode 2750, Max Tile: 8, Final Score: 2084,  Baseline: 998.45, Improvement: 1102.75\n",
      "Episode 2800, Max Tile: 6, Final Score: 572,  Baseline: 954.37, Improvement: -374.67\n",
      "Episode 2850, Max Tile: 6, Final Score: 600,  Baseline: 920.78, Improvement: -312.38\n",
      "Episode 2900, Max Tile: 7, Final Score: 1480,  Baseline: 834.58, Improvement: 661.12\n",
      "Episode 2950, Max Tile: 6, Final Score: 632,  Baseline: 835.40, Improvement: -194.70\n",
      "Episode 3000, Max Tile: 6, Final Score: 828,  Baseline: 1078.34, Improvement: -239.84\n",
      "Episode 3050, Max Tile: 6, Final Score: 504,  Baseline: 964.73, Improvement: -453.63\n",
      "Episode 3100, Max Tile: 7, Final Score: 1216,  Baseline: 854.79, Improvement: 373.91\n",
      "Episode 3150, Max Tile: 5, Final Score: 252,  Baseline: 873.12, Improvement: -616.12\n",
      "Episode 3200, Max Tile: 6, Final Score: 636,  Baseline: 899.33, Improvement: -254.73\n",
      "Episode 3250, Max Tile: 5, Final Score: 272,  Baseline: 922.32, Improvement: -645.12\n",
      "Episode 3300, Max Tile: 5, Final Score: 520,  Baseline: 946.73, Improvement: -418.83\n",
      "Episode 3350, Max Tile: 7, Final Score: 1092,  Baseline: 948.84, Improvement: 154.96\n",
      "Episode 3400, Max Tile: 7, Final Score: 1300,  Baseline: 826.08, Improvement: 487.02\n",
      "Episode 3450, Max Tile: 6, Final Score: 576,  Baseline: 822.18, Improvement: -238.38\n",
      "Episode 3500, Max Tile: 6, Final Score: 584,  Baseline: 959.15, Improvement: -367.05\n",
      "Episode 3550, Max Tile: 6, Final Score: 676,  Baseline: 865.58, Improvement: -180.58\n",
      "Episode 3600, Max Tile: 5, Final Score: 340,  Baseline: 840.95, Improvement: -495.25\n",
      "Episode 3650, Max Tile: 6, Final Score: 568,  Baseline: 911.10, Improvement: -335.40\n",
      "Episode 3700, Max Tile: 6, Final Score: 496,  Baseline: 819.58, Improvement: -316.38\n",
      "Episode 3750, Max Tile: 6, Final Score: 648,  Baseline: 865.70, Improvement: -209.10\n",
      "Episode 3800, Max Tile: 6, Final Score: 756,  Baseline: 931.68, Improvement: -165.78\n",
      "Episode 3850, Max Tile: 8, Final Score: 2268,  Baseline: 1019.64, Improvement: 1267.36\n",
      "Episode 3900, Max Tile: 7, Final Score: 1304,  Baseline: 987.06, Improvement: 330.54\n",
      "Episode 3950, Max Tile: 6, Final Score: 576,  Baseline: 913.71, Improvement: -329.81\n",
      "Episode 4000, Max Tile: 7, Final Score: 1356,  Baseline: 824.21, Improvement: 545.79\n",
      "Episode 4050, Max Tile: 7, Final Score: 1496,  Baseline: 964.75, Improvement: 546.45\n",
      "Episode 4100, Max Tile: 6, Final Score: 792,  Baseline: 961.42, Improvement: -159.72\n",
      "Episode 4150, Max Tile: 6, Final Score: 584,  Baseline: 991.52, Improvement: -399.42\n",
      "Episode 4200, Max Tile: 7, Final Score: 1564,  Baseline: 1002.68, Improvement: 577.72\n",
      "Episode 4250, Max Tile: 7, Final Score: 1336,  Baseline: 921.86, Improvement: 428.04\n",
      "Episode 4300, Max Tile: 7, Final Score: 1316,  Baseline: 1023.60, Improvement: 306.00\n",
      "Episode 4350, Max Tile: 7, Final Score: 1148,  Baseline: 911.46, Improvement: 249.14\n",
      "Episode 4400, Max Tile: 7, Final Score: 1276,  Baseline: 866.30, Improvement: 422.90\n",
      "Episode 4450, Max Tile: 6, Final Score: 688,  Baseline: 904.29, Improvement: -207.19\n",
      "Episode 4500, Max Tile: 7, Final Score: 1852,  Baseline: 972.79, Improvement: 896.81\n",
      "Episode 4550, Max Tile: 7, Final Score: 1656,  Baseline: 1048.07, Improvement: 624.63\n",
      "Episode 4600, Max Tile: 6, Final Score: 644,  Baseline: 916.60, Improvement: -263.60\n",
      "Episode 4650, Max Tile: 7, Final Score: 1308,  Baseline: 966.63, Improvement: 355.07\n",
      "Episode 4700, Max Tile: 7, Final Score: 1456,  Baseline: 1045.29, Improvement: 426.01\n",
      "Episode 4750, Max Tile: 6, Final Score: 588,  Baseline: 1007.37, Improvement: -411.57\n",
      "Episode 4800, Max Tile: 7, Final Score: 1356,  Baseline: 796.87, Improvement: 573.43\n",
      "Episode 4850, Max Tile: 5, Final Score: 352,  Baseline: 900.52, Improvement: -542.32\n",
      "Episode 4900, Max Tile: 7, Final Score: 1300,  Baseline: 1107.62, Improvement: 205.68\n",
      "Episode 4950, Max Tile: 5, Final Score: 420,  Baseline: 902.16, Improvement: -475.36\n",
      "Episode 5000, Max Tile: 6, Final Score: 628,  Baseline: 951.92, Improvement: -315.22\n",
      "Episode 5050, Max Tile: 7, Final Score: 1580,  Baseline: 989.92, Improvement: 606.68\n",
      "Episode 5100, Max Tile: 6, Final Score: 948,  Baseline: 1090.45, Improvement: -130.75\n",
      "Episode 5150, Max Tile: 5, Final Score: 424,  Baseline: 905.24, Improvement: -474.54\n",
      "Episode 5200, Max Tile: 8, Final Score: 2256,  Baseline: 1015.82, Improvement: 1259.48\n",
      "Episode 5250, Max Tile: 7, Final Score: 1252,  Baseline: 1053.26, Improvement: 211.64\n",
      "Episode 5300, Max Tile: 6, Final Score: 1008,  Baseline: 919.41, Improvement: 100.59\n",
      "Episode 5350, Max Tile: 7, Final Score: 1256,  Baseline: 1177.75, Improvement: 91.35\n",
      "Episode 5400, Max Tile: 6, Final Score: 860,  Baseline: 1160.50, Improvement: -289.30\n",
      "Episode 5450, Max Tile: 7, Final Score: 1312,  Baseline: 957.75, Improvement: 367.65\n",
      "Episode 5500, Max Tile: 7, Final Score: 1308,  Baseline: 1068.64, Improvement: 252.76\n",
      "Episode 5550, Max Tile: 6, Final Score: 888,  Baseline: 857.52, Improvement: 41.28\n",
      "Episode 5600, Max Tile: 6, Final Score: 520,  Baseline: 1103.69, Improvement: -576.39\n",
      "Episode 5650, Max Tile: 5, Final Score: 380,  Baseline: 936.54, Improvement: -550.14\n",
      "Episode 5700, Max Tile: 7, Final Score: 1368,  Baseline: 946.64, Improvement: 435.66\n",
      "Episode 5750, Max Tile: 6, Final Score: 588,  Baseline: 999.32, Improvement: -403.52\n",
      "Episode 5800, Max Tile: 6, Final Score: 624,  Baseline: 1184.90, Improvement: -552.30\n",
      "Episode 5850, Max Tile: 7, Final Score: 1480,  Baseline: 1074.71, Improvement: 420.69\n",
      "Episode 5900, Max Tile: 7, Final Score: 1180,  Baseline: 1033.68, Improvement: 158.42\n",
      "Episode 5950, Max Tile: 6, Final Score: 736,  Baseline: 964.00, Improvement: -218.20\n",
      "Episode 6000, Max Tile: 7, Final Score: 1168,  Baseline: 1030.33, Improvement: 150.57\n",
      "Episode 6050, Max Tile: 7, Final Score: 1456,  Baseline: 1146.31, Improvement: 325.29\n",
      "Episode 6100, Max Tile: 6, Final Score: 924,  Baseline: 915.17, Improvement: 20.73\n",
      "Episode 6150, Max Tile: 7, Final Score: 1660,  Baseline: 1070.12, Improvement: 606.68\n",
      "Episode 6200, Max Tile: 6, Final Score: 892,  Baseline: 1120.17, Improvement: -217.27\n",
      "Episode 6250, Max Tile: 7, Final Score: 988,  Baseline: 994.95, Improvement: 3.75\n",
      "Episode 6300, Max Tile: 8, Final Score: 2960,  Baseline: 1034.55, Improvement: 1950.05\n",
      "Episode 6350, Max Tile: 6, Final Score: 672,  Baseline: 974.56, Improvement: -293.86\n",
      "Episode 6400, Max Tile: 8, Final Score: 2040,  Baseline: 1095.40, Improvement: 961.80\n",
      "Episode 6450, Max Tile: 6, Final Score: 560,  Baseline: 1046.86, Improvement: -479.26\n",
      "Episode 6500, Max Tile: 6, Final Score: 720,  Baseline: 1002.97, Improvement: -272.97\n",
      "Episode 6550, Max Tile: 8, Final Score: 2208,  Baseline: 1110.49, Improvement: 1116.21\n",
      "Episode 6600, Max Tile: 8, Final Score: 2200,  Baseline: 1073.12, Improvement: 1144.68\n",
      "Episode 6650, Max Tile: 5, Final Score: 500,  Baseline: 1058.64, Improvement: -551.04\n",
      "Episode 6700, Max Tile: 6, Final Score: 628,  Baseline: 1002.26, Improvement: -365.56\n",
      "Episode 6750, Max Tile: 5, Final Score: 396,  Baseline: 945.28, Improvement: -542.68\n",
      "Episode 6800, Max Tile: 7, Final Score: 1700,  Baseline: 1009.68, Improvement: 707.32\n",
      "Episode 6850, Max Tile: 6, Final Score: 712,  Baseline: 1030.08, Improvement: -308.48\n",
      "Episode 6900, Max Tile: 6, Final Score: 1028,  Baseline: 1136.26, Improvement: -95.66\n",
      "Episode 6950, Max Tile: 6, Final Score: 476,  Baseline: 981.21, Improvement: -498.31\n",
      "Episode 7000, Max Tile: 7, Final Score: 1584,  Baseline: 1130.69, Improvement: 469.11\n",
      "Episode 7050, Max Tile: 7, Final Score: 1288,  Baseline: 1142.26, Improvement: 158.94\n",
      "Episode 7100, Max Tile: 6, Final Score: 1136,  Baseline: 1154.47, Improvement: -5.17\n",
      "Episode 7150, Max Tile: 6, Final Score: 664,  Baseline: 1139.38, Improvement: -466.18\n",
      "Episode 7200, Max Tile: 7, Final Score: 1320,  Baseline: 1235.53, Improvement: 98.17\n",
      "Episode 7250, Max Tile: 6, Final Score: 924,  Baseline: 1094.37, Improvement: -158.77\n",
      "Episode 7300, Max Tile: 7, Final Score: 1268,  Baseline: 1133.20, Improvement: 148.20\n",
      "Episode 7350, Max Tile: 7, Final Score: 1088,  Baseline: 1107.93, Improvement: -8.53\n",
      "Episode 7400, Max Tile: 7, Final Score: 1704,  Baseline: 1248.06, Improvement: 473.04\n",
      "Episode 7450, Max Tile: 6, Final Score: 1188,  Baseline: 1190.41, Improvement: 11.69\n",
      "Episode 7500, Max Tile: 7, Final Score: 1464,  Baseline: 1193.95, Improvement: 285.15\n",
      "Episode 7550, Max Tile: 6, Final Score: 884,  Baseline: 1216.95, Improvement: -321.25\n",
      "Episode 7600, Max Tile: 8, Final Score: 2176,  Baseline: 1204.97, Improvement: 989.43\n",
      "Episode 7650, Max Tile: 7, Final Score: 1048,  Baseline: 1185.49, Improvement: -125.99\n",
      "Episode 7700, Max Tile: 6, Final Score: 1148,  Baseline: 1114.35, Improvement: 46.55\n",
      "Episode 7750, Max Tile: 5, Final Score: 304,  Baseline: 1008.99, Improvement: -699.39\n",
      "Episode 7800, Max Tile: 8, Final Score: 2228,  Baseline: 1011.48, Improvement: 1235.92\n",
      "Episode 7850, Max Tile: 6, Final Score: 972,  Baseline: 1307.50, Improvement: -324.30\n",
      "Episode 7900, Max Tile: 7, Final Score: 2148,  Baseline: 1348.83, Improvement: 819.57\n",
      "Episode 7950, Max Tile: 6, Final Score: 672,  Baseline: 1084.28, Improvement: -402.98\n",
      "Episode 8000, Max Tile: 7, Final Score: 1168,  Baseline: 1047.93, Improvement: 132.97\n",
      "Episode 8050, Max Tile: 6, Final Score: 644,  Baseline: 1242.40, Improvement: -589.70\n",
      "Episode 8100, Max Tile: 7, Final Score: 1496,  Baseline: 1133.25, Improvement: 378.05\n",
      "Episode 8150, Max Tile: 7, Final Score: 1396,  Baseline: 1056.19, Improvement: 354.01\n",
      "Episode 8200, Max Tile: 6, Final Score: 672,  Baseline: 1181.67, Improvement: -500.77\n",
      "Episode 8250, Max Tile: 7, Final Score: 1828,  Baseline: 1179.43, Improvement: 666.07\n",
      "Episode 8300, Max Tile: 6, Final Score: 1048,  Baseline: 1435.94, Improvement: -375.24\n",
      "Episode 8350, Max Tile: 6, Final Score: 828,  Baseline: 1428.02, Improvement: -588.92\n",
      "Episode 8400, Max Tile: 5, Final Score: 268,  Baseline: 1096.14, Improvement: -822.94\n",
      "Episode 8450, Max Tile: 6, Final Score: 588,  Baseline: 1287.46, Improvement: -691.26\n",
      "Episode 8500, Max Tile: 8, Final Score: 3224,  Baseline: 1349.45, Improvement: 1900.45\n",
      "Episode 8550, Max Tile: 8, Final Score: 3064,  Baseline: 1301.41, Improvement: 1787.69\n",
      "Episode 8600, Max Tile: 8, Final Score: 2180,  Baseline: 1506.96, Improvement: 691.64\n",
      "Episode 8650, Max Tile: 7, Final Score: 1728,  Baseline: 1332.05, Improvement: 413.25\n",
      "Episode 8700, Max Tile: 7, Final Score: 1228,  Baseline: 1322.63, Improvement: -82.03\n",
      "Episode 8750, Max Tile: 8, Final Score: 2516,  Baseline: 1350.20, Improvement: 1186.80\n",
      "Episode 8800, Max Tile: 7, Final Score: 1108,  Baseline: 1218.21, Improvement: -98.01\n",
      "Episode 8850, Max Tile: 8, Final Score: 3752,  Baseline: 1382.97, Improvement: 2397.03\n",
      "Episode 8900, Max Tile: 5, Final Score: 484,  Baseline: 1310.51, Improvement: -818.71\n",
      "Episode 8950, Max Tile: 7, Final Score: 1456,  Baseline: 1274.73, Improvement: 196.47\n",
      "Episode 9000, Max Tile: 7, Final Score: 1308,  Baseline: 1391.02, Improvement: -69.02\n",
      "Episode 9050, Max Tile: 5, Final Score: 312,  Baseline: 1500.95, Improvement: -1183.25\n",
      "Episode 9100, Max Tile: 6, Final Score: 732,  Baseline: 1088.19, Improvement: -346.59\n",
      "Episode 9150, Max Tile: 5, Final Score: 296,  Baseline: 1363.22, Improvement: -1061.82\n",
      "Episode 9200, Max Tile: 7, Final Score: 1376,  Baseline: 1343.75, Improvement: 46.65\n",
      "Episode 9250, Max Tile: 6, Final Score: 836,  Baseline: 1281.02, Improvement: -434.62\n",
      "Episode 9300, Max Tile: 6, Final Score: 900,  Baseline: 1264.56, Improvement: -352.56\n",
      "Episode 9350, Max Tile: 6, Final Score: 868,  Baseline: 1122.43, Improvement: -243.23\n",
      "Episode 9400, Max Tile: 7, Final Score: 1472,  Baseline: 1351.62, Improvement: 135.98\n",
      "Episode 9450, Max Tile: 8, Final Score: 2724,  Baseline: 1390.77, Improvement: 1356.53\n",
      "Episode 9500, Max Tile: 7, Final Score: 1240,  Baseline: 1233.02, Improvement: 19.78\n",
      "Episode 9550, Max Tile: 8, Final Score: 3288,  Baseline: 1396.91, Improvement: 1918.29\n",
      "Episode 9600, Max Tile: 6, Final Score: 648,  Baseline: 1219.07, Improvement: -561.97\n",
      "Episode 9650, Max Tile: 6, Final Score: 500,  Baseline: 1482.67, Improvement: -975.37\n",
      "Episode 9700, Max Tile: 7, Final Score: 1304,  Baseline: 1363.88, Improvement: -46.68\n",
      "Episode 9750, Max Tile: 7, Final Score: 1252,  Baseline: 1367.10, Improvement: -101.70\n",
      "Episode 9800, Max Tile: 8, Final Score: 3196,  Baseline: 1471.65, Improvement: 1750.55\n",
      "Episode 9850, Max Tile: 6, Final Score: 944,  Baseline: 1428.15, Improvement: -472.55\n",
      "Episode 9900, Max Tile: 7, Final Score: 1484,  Baseline: 1228.77, Improvement: 270.63\n",
      "Episode 9950, Max Tile: 7, Final Score: 1288,  Baseline: 1384.71, Improvement: -83.71\n",
      "Episode 10000, Max Tile: 4, Final Score: 152,  Baseline: 1045.56, Improvement: -889.76\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "model.eval()\n",
    "env = Game2048Env()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_episodes = 10000\n",
    "baseline = 1500\n",
    "batch_size = 25\n",
    "batch_log_probs = []\n",
    "batch_improvs = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    valid_log_probs = []\n",
    "    finalscore = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "        logits = model(state_tensor)\n",
    "\n",
    "        ranked_actions = torch.argsort(logits, dim=1, descending=True)[0]   #sort by how liikely move is\n",
    "\n",
    "        original_board = env.board.copy()\n",
    "        final_action = None\n",
    "        selected_log_prob = None\n",
    "\n",
    "        movecount = 0   #tracks if the first move was valid \n",
    "        for action in ranked_actions:\n",
    "            test_board, _ = move(original_board.copy(), action.item())\n",
    "            if not np.array_equal(test_board, original_board):\n",
    "                final_action = action.item()\n",
    "                dist = Categorical(logits=logits)\n",
    "                selected_log_prob = dist.log_prob(action)\n",
    "                break\n",
    "            movecount += 1\n",
    "\n",
    "        if final_action is None:    #game is stuck, skip (shouldn't happen)\n",
    "            print(\"SOMETHING WRONG AAAAAUUEEUAGHGEUGHHH\")\n",
    "            break\n",
    "\n",
    "        state, score, done = env.step(final_action)\n",
    "        valid_log_probs.append(selected_log_prob)\n",
    "        finalscore += score\n",
    "\n",
    "    # use baseline to force games to improve\n",
    "    baseline = 0.95 * baseline + 0.05 * finalscore\n",
    "    improvement= finalscore - baseline + 0.1 * len(valid_log_probs)\n",
    "\n",
    "    #use batches for more stable training\n",
    "    batch_log_probs.extend(valid_log_probs)\n",
    "    batch_improvs.extend([improvement] * len(valid_log_probs))\n",
    "\n",
    "    if (episode+1) % batch_size == 0:\n",
    "        loss = 0\n",
    "        for log_prob, improvement in zip(batch_log_probs, batch_improvs):\n",
    "            loss -= log_prob * improvement\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_log_probs = []\n",
    "        batch_improvs = []\n",
    "\n",
    "    if (episode+1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1}, Max Tile: {env.board.max()}, Final Score: {finalscore},  Baseline: {baseline:.2f}, Improvement: {improvement:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Finetuned to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "dummy_input = torch.randn(1, 1, 4, 4) #batch, channels, height, width for cnn\n",
    "torch.onnx.export(model, dummy_input, \"2048_fine.onnx\", input_names=[\"input\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Finetuned ONNX to Tensorflow to Tensorflow.js\n",
    "\n",
    "you know the drill...\n",
    "\n",
    "make a new virtual env with the following:\\\n",
    "`python3.9 -m venv tfenv`\n",
    "\n",
    "run this bash script to convert to Tensorflow:\\\n",
    "`pip install tensorflow==2.13.0 keras==2.13.1 onnx==1.14.0 onnx-tf==1.10.0 protobuf==3.20.3 tensorflow-probability==0.20.0 && onnx-tf convert -i 2048_fine.onnx -o 2048_fine_tf`\n",
    "\n",
    "and then this bash script to convert from Tensorflow to Tensorflow.js:\\\n",
    "`pip install tensorflowjs==4.18.0 && tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model 2048_fine_tf/ 2048_fine_tfjs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
